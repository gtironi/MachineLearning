{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a4c9d77-8ce0-4600-b5db-f98f39c8e9a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2163fb6c-8186-42af-885f-8278e7ca2ce3",
   "metadata": {},
   "source": [
    "# Tarefa 4 - Decision Trees, Random Forest and K-Means\n",
    "Fourth assessed coursework for the course: Técnicas e Algoritmos em Ciência de Dados\n",
    "\n",
    "This tarefa provides an exciting opportunity for students to put their knowledge acquired in class into practice, using decision trees and random forests to solve a real-world problem in classification and delve into the world of unsupervised learning by implementing the K-means algorithm. Students will also get used to generating important plots during training to analyse the models' behaviour. \n",
    "\n",
    "## General guidelines:\n",
    "\n",
    "* This work must be entirely original. You are allowed to research documentation for specific libraries, but copying solutions from the internet or your classmates is strictly prohibited. Any such actions will result in a deduction of points for the coursework.\n",
    "* Before submitting your work, make sure to rename the file to the random number that you created for the previous coursework (for example, 289479.ipynb).\n",
    "\n",
    "## Notebook Overview:\n",
    "\n",
    "1. [Decision Trees](#Decision_Trees) (30%)\n",
    "2. [Random Forest](#Random_Forest) (30%)\n",
    "3. [K-Means](#K-Means) (30%)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698bb261-de78-4c09-bbff-f1128567f078",
   "metadata": {},
   "source": [
    "### Decision_Trees\n",
    "## Part 1 - Decision Trees for Classification (value: 30%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c3fc47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "ddi_df = pd.read_csv('data/ddi_dt_sample.csv') # don't forget to change the path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e9c360d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>drug1</th>\n",
       "      <th>drug2</th>\n",
       "      <th>ddi</th>\n",
       "      <th>pca_max_0</th>\n",
       "      <th>pca_max_1</th>\n",
       "      <th>pca_max_2</th>\n",
       "      <th>pca_max_3</th>\n",
       "      <th>pca_max_4</th>\n",
       "      <th>pca_max_5</th>\n",
       "      <th>pca_max_6</th>\n",
       "      <th>...</th>\n",
       "      <th>pca_min_17</th>\n",
       "      <th>pca_min_18</th>\n",
       "      <th>pca_min_19</th>\n",
       "      <th>pca_min_20</th>\n",
       "      <th>pca_min_21</th>\n",
       "      <th>pca_min_22</th>\n",
       "      <th>pca_min_23</th>\n",
       "      <th>pca_min_24</th>\n",
       "      <th>pca_min_25</th>\n",
       "      <th>pca_min_26</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CID000002725</td>\n",
       "      <td>CID000013342</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.383210</td>\n",
       "      <td>-0.066904</td>\n",
       "      <td>0.051153</td>\n",
       "      <td>0.058145</td>\n",
       "      <td>0.398508</td>\n",
       "      <td>0.215271</td>\n",
       "      <td>-0.267814</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.36635</td>\n",
       "      <td>-1.091191</td>\n",
       "      <td>-1.021989</td>\n",
       "      <td>-0.086979</td>\n",
       "      <td>-0.326333</td>\n",
       "      <td>-0.17953</td>\n",
       "      <td>-0.162906</td>\n",
       "      <td>-0.204593</td>\n",
       "      <td>-0.09133</td>\n",
       "      <td>-0.195522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CID000002802</td>\n",
       "      <td>CID000013342</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.554958</td>\n",
       "      <td>-0.066904</td>\n",
       "      <td>0.053715</td>\n",
       "      <td>0.058145</td>\n",
       "      <td>0.398508</td>\n",
       "      <td>0.131852</td>\n",
       "      <td>-0.267814</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.36635</td>\n",
       "      <td>0.106596</td>\n",
       "      <td>0.010149</td>\n",
       "      <td>-0.086979</td>\n",
       "      <td>-0.326333</td>\n",
       "      <td>-0.17953</td>\n",
       "      <td>-0.162906</td>\n",
       "      <td>-0.204593</td>\n",
       "      <td>-0.08010</td>\n",
       "      <td>-0.195522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CID000002083</td>\n",
       "      <td>CID000013342</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.908402</td>\n",
       "      <td>-0.066904</td>\n",
       "      <td>0.122384</td>\n",
       "      <td>0.058145</td>\n",
       "      <td>0.398508</td>\n",
       "      <td>0.098734</td>\n",
       "      <td>0.041276</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.36635</td>\n",
       "      <td>0.102901</td>\n",
       "      <td>0.010149</td>\n",
       "      <td>-0.088365</td>\n",
       "      <td>-0.326333</td>\n",
       "      <td>-0.17953</td>\n",
       "      <td>-0.162906</td>\n",
       "      <td>-0.204593</td>\n",
       "      <td>-0.08010</td>\n",
       "      <td>-0.195522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CID000004034</td>\n",
       "      <td>CID000013342</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.547375</td>\n",
       "      <td>-0.066904</td>\n",
       "      <td>0.051884</td>\n",
       "      <td>0.058145</td>\n",
       "      <td>0.398508</td>\n",
       "      <td>0.203962</td>\n",
       "      <td>-0.267814</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.36635</td>\n",
       "      <td>-1.011102</td>\n",
       "      <td>-0.944133</td>\n",
       "      <td>-0.086979</td>\n",
       "      <td>-0.326333</td>\n",
       "      <td>-0.17953</td>\n",
       "      <td>-0.162906</td>\n",
       "      <td>-0.204593</td>\n",
       "      <td>-0.08010</td>\n",
       "      <td>-0.195522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CID000003899</td>\n",
       "      <td>CID000013342</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.554958</td>\n",
       "      <td>-0.066904</td>\n",
       "      <td>0.051083</td>\n",
       "      <td>0.058145</td>\n",
       "      <td>0.398508</td>\n",
       "      <td>0.138126</td>\n",
       "      <td>-0.267814</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.36635</td>\n",
       "      <td>0.075010</td>\n",
       "      <td>0.010149</td>\n",
       "      <td>-0.086979</td>\n",
       "      <td>-0.326333</td>\n",
       "      <td>-0.17953</td>\n",
       "      <td>-0.162906</td>\n",
       "      <td>-0.204593</td>\n",
       "      <td>-0.08010</td>\n",
       "      <td>-0.195522</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 57 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          drug1         drug2  ddi  pca_max_0  pca_max_1  pca_max_2  \\\n",
       "0  CID000002725  CID000013342  0.0  -2.383210  -0.066904   0.051153   \n",
       "1  CID000002802  CID000013342  0.0  -2.554958  -0.066904   0.053715   \n",
       "2  CID000002083  CID000013342  0.0   9.908402  -0.066904   0.122384   \n",
       "3  CID000004034  CID000013342  0.0  -2.547375  -0.066904   0.051884   \n",
       "4  CID000003899  CID000013342  0.0  -2.554958  -0.066904   0.051083   \n",
       "\n",
       "   pca_max_3  pca_max_4  pca_max_5  pca_max_6  ...  pca_min_17  pca_min_18  \\\n",
       "0   0.058145   0.398508   0.215271  -0.267814  ...    -0.36635   -1.091191   \n",
       "1   0.058145   0.398508   0.131852  -0.267814  ...    -0.36635    0.106596   \n",
       "2   0.058145   0.398508   0.098734   0.041276  ...    -0.36635    0.102901   \n",
       "3   0.058145   0.398508   0.203962  -0.267814  ...    -0.36635   -1.011102   \n",
       "4   0.058145   0.398508   0.138126  -0.267814  ...    -0.36635    0.075010   \n",
       "\n",
       "   pca_min_19  pca_min_20  pca_min_21  pca_min_22  pca_min_23  pca_min_24  \\\n",
       "0   -1.021989   -0.086979   -0.326333    -0.17953   -0.162906   -0.204593   \n",
       "1    0.010149   -0.086979   -0.326333    -0.17953   -0.162906   -0.204593   \n",
       "2    0.010149   -0.088365   -0.326333    -0.17953   -0.162906   -0.204593   \n",
       "3   -0.944133   -0.086979   -0.326333    -0.17953   -0.162906   -0.204593   \n",
       "4    0.010149   -0.086979   -0.326333    -0.17953   -0.162906   -0.204593   \n",
       "\n",
       "   pca_min_25  pca_min_26  \n",
       "0    -0.09133   -0.195522  \n",
       "1    -0.08010   -0.195522  \n",
       "2    -0.08010   -0.195522  \n",
       "3    -0.08010   -0.195522  \n",
       "4    -0.08010   -0.195522  \n",
       "\n",
       "[5 rows x 57 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inspect the data\n",
    "ddi_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf94b3a6",
   "metadata": {},
   "source": [
    "As you can observe, the first 2 columns represent the IDs of the drugs in each combination. The 3rd column represents the binary label indicating if the pair causes an adverse interaction or not. The remaining 54 columns are the features based on the PCA representations of individual drug targets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7438a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions\n",
    "\n",
    "###----- criterions -----###\n",
    "def se_criterion(region):\n",
    "    \"\"\"\n",
    "    Implements the sum of squared error criterion in a region\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    region : ndarray\n",
    "        Array of shape (N,) containing the values of the target values \n",
    "        for N datapoints in the training set.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        The sum of squared error\n",
    "        \n",
    "    Note\n",
    "    ----\n",
    "    The error for an empty region should be infinity (use: float(\"inf\"))\n",
    "    This avoids creating empty regions\n",
    "    \"\"\"\n",
    "    # your code here\n",
    "    if len(region) == 0:\n",
    "        return float(\"inf\")\n",
    "    \n",
    "    if isinstance(region, pd.DataFrame) or isinstance(region, pd.Series):\n",
    "        region = region.values\n",
    "    \n",
    "    mean_value = np.mean(region)\n",
    "    squared_errors = (region - mean_value) ** 2\n",
    "    return np.sum(squared_errors)\n",
    "\n",
    "#Classes\n",
    "class DecisionTree:\n",
    "    def __init__(self, criterion = se_criterion, max_depth=4, min_samples_leaf=1):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.criterion = criterion\n",
    "        self.tree = None\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        train_data = np.column_stack((X_train, y_train))  # \n",
    "        self.tree = self.create_tree(train_data, 0)  # \n",
    "\n",
    "    def predict(self, X_test):\n",
    "        predictions = np.array([self._predict_sample(x, self.tree) for x in X_test])\n",
    "        return predictions\n",
    "    \n",
    "    def print_tree(self):\n",
    "        self._print_tree(self.tree)\n",
    "\n",
    "    def create_tree(self, data, current_depth):\n",
    "        if current_depth > self.max_depth:\n",
    "            return None\n",
    "        \n",
    "        split_data, split_feature_idx, split_feature_val = self.find_bestsplit(data)\n",
    "\n",
    "        node = TreeNode(data, split_feature_idx, split_feature_val, self.criterion(data[:,-1])/data.shape[0])\n",
    "        \n",
    "        if split_data is None: #\n",
    "            return node\n",
    "        \n",
    "        if len(split_data[0]) <= self.min_samples_leaf or len(split_data[1]) <= self.min_samples_leaf:\n",
    "            return node\n",
    "\n",
    "        current_depth += 1\n",
    "        node.left = self.create_tree(split_data[0], current_depth)\n",
    "        node.right = self.create_tree(split_data[1], current_depth)\n",
    "\n",
    "        return node\n",
    "\n",
    "    def find_bestsplit(self, data):\n",
    "        if isinstance(data, pd.DataFrame):\n",
    "            data = data.values\n",
    "        if isinstance(data, pd.Series):\n",
    "            data = data.values\n",
    "        \n",
    "        best_sq_error = float('inf')\n",
    "        best_feature_index = None\n",
    "        best_tau = None\n",
    "        regions = None\n",
    "\n",
    "        _, n_features = data.shape\n",
    "        \n",
    "        for feature_index in range(n_features - 1): \n",
    "            feature_values = data[:, feature_index]\n",
    "            possible_taus = np.unique(feature_values) #\n",
    "            \n",
    "            for tau in possible_taus:\n",
    "                left, right = self.split(data, feature_index, tau)\n",
    "                sq_error = self.criterion(left[:, -1]) + self.criterion(right[:, -1])\n",
    "                \n",
    "                if sq_error < best_sq_error and\\\n",
    "                len(left)>self.min_samples_leaf and\\\n",
    "                len(right)>self.min_samples_leaf: #\n",
    "                    best_sq_error = sq_error\n",
    "                    best_feature_index = feature_index\n",
    "                    best_tau = tau\n",
    "                    regions = (left, right)\n",
    "\n",
    "        return regions, best_feature_index, best_tau\n",
    "\n",
    "    def split(self, data, feature_index, tau, indices = False):\n",
    "        if isinstance(data, pd.DataFrame):\n",
    "            feature_values = data.iloc[:, feature_index].values\n",
    "        else:\n",
    "            feature_values = data[:, feature_index]\n",
    "            \n",
    "        left_indices = np.where(feature_values <= tau)[0] \n",
    "        right_indices = np.where(feature_values > tau)[0] \n",
    "        \n",
    "        if indices: #\n",
    "            return left_indices, right_indices\n",
    "        \n",
    "        #\n",
    "        left_partition = data[left_indices]\n",
    "        right_partition = data[right_indices]\n",
    "        \n",
    "        return left_partition, right_partition\n",
    "\n",
    "    def _predict_sample(self, sample, node):\n",
    "        if node.is_leaf:\n",
    "            return np.mean(node.data[:, -1]) # \n",
    "        if sample[node.feature_idx] < node.feature_val:\n",
    "            return self._predict_sample(sample, node.left)\n",
    "        else:\n",
    "            return self._predict_sample(sample, node.right)\n",
    "\n",
    "    def _print_tree(self, node, depth=0, prefix=\"\"):\n",
    "        if node is None:\n",
    "            return\n",
    "        indent = \"  \" * depth\n",
    "        if node.is_leaf:\n",
    "            print(f\"{indent}{prefix}Leaf: Predict {np.mean(node.data[:, -1]):.2f}, Samples: {len(node.data)}, sq_error = {node.sq_error:.2f}\")\n",
    "        else:\n",
    "            print(f\"{indent}{prefix}Node: Feature {node.feature_idx}, Threshold {node.feature_val:.2f}, sq_error = {node.sq_error:.2f}\")\n",
    "            self._print_tree(node.left, depth + 1, prefix=\"L--> \")\n",
    "            self._print_tree(node.right, depth + 1, prefix=\"R--> \")\n",
    "\n",
    "class TreeNode:\n",
    "    def __init__(self, data, feature_idx, feature_val, sq_error):\n",
    "        self.data = data\n",
    "        self.feature_idx = feature_idx\n",
    "        self.feature_val = feature_val\n",
    "        self.sq_error = sq_error\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "\n",
    "    @property\n",
    "    def is_leaf(self):\n",
    "        return self.left is None and self.right is None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51329343-9ecb-4e89-8b7b-bdfd62c4d777",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## your code goes here:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43a1622-54da-4aa8-a1c2-00f3241292be",
   "metadata": {},
   "source": [
    "## Random_Forest\n",
    "## Part 2 - Random Forest for Classification Networks (value: 30%)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ae13d9f-4ef0-4735-8190-1bcaaa28ea2e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## your code goes here:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2950e57-8657-4066-b657-1c96c830ce6e",
   "metadata": {},
   "source": [
    "## K-Means\n",
    "## Part 3 – Clustering with K-means (value: 40%)\n",
    "\n",
    "In this exercise, you will explore clustering by implementing the K-means algorithm. You will write code to perform K-means clustering while visualizing the movement of the centroids at each iteration. \n",
    "\n",
    "To complete this exercise, you will write code to implement K-means for clustering: \n",
    "\n",
    "1. Dataset Preparation: Run the cells provided in the notebook that generate the artificial data points for this exercise.\n",
    "2. K-means Clustering:\n",
    "\t- Initialize K cluster centroids by selecting K points from your dataset at random.\n",
    "\t- Implement a loop to perform the following steps until convergence (or until a specified maximum number of iterations is reached, e.g., 150):\n",
    "        - Assign each data point to the nearest centroid (you will have to calculate the Euclidean distance between the data point and each centroid).\n",
    "        - Update each centroid by moving it to the mean of all data points assigned to it.\n",
    "        - Check for convergence by comparing the new centroids with the previous centroids. If the difference is smaller than an $\\epsilon=1^{-4}$, exit the loop.\n",
    "3. Centroid Movement Visualization:\n",
    "\t- At 5 different moments during training, plot a figure showing the centroids and the points. Figure 1 should show the situation at the beginning, before learning. Figure 5 should show the situation at the end of the learning. The remaining Figures 2-4 should show intermediary situations.\n",
    "\t- For each figure, each centroid will be represented by a large black cross and each cluster with a different colour, the points must be coloured according to their respective cluster.\n",
    "4. Sum of squared distances:\n",
    "\t- Along with plotting the centroid movement, calculate the sum of squared distances at each iteration as follows:\n",
    "        - $\\sum_{j=1}^K \\sum_{n \\in S_j}d(x_n,\\mu_j )^2$, where $K$ is the number of clusters, $x_n$ represents the $n^{th}$ datapoint, $n \\in S_j$ indicates a set of points that belong to cluster $S_j$, $\\mu_j$ is the mean of the datapoints in $S_j$ and $d(x_n,\\mu_j)$ indicates the Euclidean distance between $x_n$ and $\\mu_j$.\n",
    "\t- Make a plot of the sum of squared distances at each iteration. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a03418cb-b1b2-4fd8-b0e9-6d1a21ae5882",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate artificial data points\n",
    "np.random.seed(13)\n",
    "num_samples = 200\n",
    "num_features = 2\n",
    "X = np.random.randn(num_samples, num_features) * 1.5 + np.array([[2, 2]])\n",
    "X = np.concatenate([X, np.random.randn(num_samples, num_features) * 3 + np.array([[-5, -5]])])\n",
    "X = np.concatenate([X, np.random.randn(num_samples, num_features) * 2 + np.array([[7, -5]])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "47b782b9-0bd4-4085-a5f2-f55ca13208eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code goes here:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
