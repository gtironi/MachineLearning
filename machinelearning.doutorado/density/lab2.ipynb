{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2: Density Estimation\n",
    "\n",
    "Import and explore the data, implements python functions to (i) estimate density distributions from the data, and (ii) predict classes using the estimated density distributions. Submit this notebook to eclass with the name \"lab1_123456.ipynb\" and replace \"123456\" by the ID you used for the first lab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the libraries you will use for this lab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.io\n",
    "from sklearn import model_selection\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the data in \"DensitiEstimationDataset.mat\" and \"GlassClassification.csv\" datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = scipy.io.loadmat('DensityEstimationDataset.mat') #read scipy.io.loadmat() documentation if necessary, check what are you exactly importing here\n",
    "#you can use pandas to import GlassClassification.csv\n",
    "X1 = mat['DensityEstimationDataset'][:, :2]\n",
    "y1 = mat['DensityEstimationDataset'][:, 2].astype(int).flatten()\n",
    "\n",
    "df = pd.read_csv('GlassClassification.csv')\n",
    "\n",
    "X2 = df.iloc[:, :4].values\n",
    "y2 = df.iloc[:, 4].astype(int).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the classes for each of the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Para o .mat: {np.int64(0), np.int64(1), np.int64(2)}; Para o Glass: {np.int64(1), np.int64(2)}'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"Para o .mat: {set(y1)}; Para o Glass: {set(y2)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Divide your dataset into training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X1_train, X1_test_val, y1_train, y1_test_val = train_test_split(X1, y1, train_size=0.80, random_state=42)\n",
    "X1_test, X1_val, y1_test, y1_val = train_test_split(X1_test_val, y1_test_val, test_size=0.5, random_state=42)\n",
    "\n",
    "X2_train, X2_test_val, y2_train, y2_test_val = train_test_split(X2, y2, train_size=0.80, random_state=42)\n",
    "X2_test, X2_val, y2_test, y2_val = train_test_split(X2_test_val, y2_test_val, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a function that estimate gaussian distributions for each class. Think that the output your function generates needs to be useful for you to predict for new points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GDE(Xtrain, ytrain):\n",
    "    \"\"\"\n",
    "    Estimates Gaussian parameters (mean, covariance, and prior) for each class.\n",
    "\n",
    "    Parameters:\n",
    "      Xtrain : ndarray of shape (n_samples, n_features)\n",
    "      ytrain : ndarray of shape (n_samples,)\n",
    "\n",
    "    Returns:\n",
    "      model : dict with keys equal to class labels and values equal to tuples (mu, cov, prior)\n",
    "    \"\"\"\n",
    "    classes = np.unique(ytrain)\n",
    "    models = {}\n",
    "    n_samples = Xtrain.shape[0]\n",
    "\n",
    "    for c in classes:\n",
    "        # select points that belong to class c\n",
    "        indices = (ytrain == c)\n",
    "        Xc = Xtrain[indices]\n",
    "\n",
    "        # estimate mean\n",
    "        mu = np.mean(Xc, axis=0)\n",
    "\n",
    "        # estimate variance (or covariance)\n",
    "        cov = np.cov(Xc, rowvar=False)\n",
    "\n",
    "        # estimate prob for the class\n",
    "        p_class = Xc.shape[0] / float(n_samples)  # relative frequency as the class prior\n",
    "\n",
    "        models[c] = (mu, cov, p_class)\n",
    "\n",
    "    return models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analize the output of your function and built another one that can use that output to predict the class for a new set of points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MGaussian(x, mu, cov):\n",
    "    \"\"\"\n",
    "    Computes the multivariate Gaussian probability density for each row in X.\n",
    "\n",
    "    Parameters:\n",
    "      x   : ndarray of shape (n_features,) the feature vectors\n",
    "      mu  : ndarray of shape (n_features,) the mean vector\n",
    "      cov : ndarray of shape (n_features, n_features) the covariance matrix (or sigma**2)\n",
    "\n",
    "    Returns:\n",
    "      pdf_values : ndarray of shape (n_samples,)\n",
    "    \"\"\"\n",
    "\n",
    "    # constant term\n",
    "    d = x.shape[0]\n",
    "\n",
    "    cov_inv = np.linalg.inv(cov)\n",
    "    det_cov = np.linalg.det(cov)\n",
    "    norm_const = 1. / np.sqrt((2 * np.pi)**d * det_cov)\n",
    "\n",
    "    # exponent term\n",
    "    diff = x - mu\n",
    "\n",
    "    diff = diff.reshape(len(diff), 1)\n",
    "\n",
    "    exponent = -0.5 * (diff.T @ cov_inv @ diff)\n",
    "\n",
    "    # compute the pdf\n",
    "    pdf_value = norm_const * np.exp(exponent)\n",
    "\n",
    "    return pdf_value\n",
    "\n",
    "def predG(model, Xtest):\n",
    "    \"\"\"\n",
    "    Predicts the class label for each test example using the Gaussian model.\n",
    "\n",
    "    Parameters:\n",
    "      model : output of the GDE function (dict mapping classes to parameters)\n",
    "      Xtest : ndarray of shape (n_test_samples, n_features)\n",
    "\n",
    "    Returns:\n",
    "      predictions : ndarray of predicted class labels of length n_test_samples\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "\n",
    "    for x in Xtest:\n",
    "        class_scores = {}\n",
    "        # for each class, calculate the bayes teorem\n",
    "        for c in model.keys():\n",
    "            mu, cov, p_class = model[c]\n",
    "            # use MGaussian to calculate p(x|c)\n",
    "            pdf_val = MGaussian(x, mu, cov)[0]\n",
    "\n",
    "            # numerator of Bayes theorem (we don't need to calculate the denominator, it is the same for each class)\n",
    "            class_scores[c] = pdf_val * p_class\n",
    "\n",
    "        # choose the class with maximum p(c|x)\n",
    "        predicted_class = max(class_scores, key=class_scores.get)\n",
    "        predictions.append(predicted_class)\n",
    "\n",
    "    return np.array(predictions)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a function that predicts the class of a point using the histogram of the points in the training set. This function does not \"learn\" a model, is directly a predictor of the point class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predH(Xtrain, ytrain, point, nbins):\n",
    "    \"\"\"\n",
    "    Predicts the class label for a given point using a histogram-based approach.\n",
    "    The feature space is divided into 'nbins' bins per dimension.\n",
    "\n",
    "    Parameters:\n",
    "      Xtrain : ndarray of shape (n_samples, n_features) used to build the histogram\n",
    "      ytrain : ndarray of shape (n_samples,) with training class labels\n",
    "      point  : ndarray of shape (n_features,) for which the class is predicted\n",
    "      nbins  : int, number of bins for each feature dimension\n",
    "\n",
    "    Returns:\n",
    "      predicted_class : the predicted class label for 'point'\n",
    "    \"\"\"\n",
    "    n_features = Xtrain.shape[1]\n",
    "    bins = []  # Will contain bin edges for each dimension\n",
    "\n",
    "    # Calculate equal-width bin edges for each feature using the training data.\n",
    "    for j in range(n_features):\n",
    "        min_val = np.min(Xtrain[:, j])\n",
    "        max_val = np.max(Xtrain[:, j])\n",
    "        # Create nbins+1 edges (this defines nbins bins)\n",
    "        edges = np.linspace(min_val, max_val, nbins + 1)\n",
    "        bins.append(edges)\n",
    "\n",
    "    bins = np.array(bins)  # Shape: (n_features, nbins+1)\n",
    "\n",
    "    # Determine the bin index for each dimension of the 'point'\n",
    "    point_bin = []\n",
    "    for j in range(n_features):\n",
    "        # np.digitize returns indices starting at 1; subtract 1 for 0-based index.\n",
    "        bin_index = np.digitize(point[j], bins[j]) - 1\n",
    "        # Adjust if point equals the maximum value\n",
    "        if bin_index == nbins:\n",
    "            bin_index = nbins - 1\n",
    "        point_bin.append(bin_index)\n",
    "    point_bin = np.array(point_bin)\n",
    "\n",
    "    # Determine the bin indices for each point in the training set for each feature.\n",
    "    Xbins = []\n",
    "    for j in range(n_features):\n",
    "        # Again subtract 1 to get 0-based indices.\n",
    "        col_bins = np.digitize(Xtrain[:, j], bins[j]) - 1\n",
    "        Xbins.append(col_bins)\n",
    "    # Stack to form an array of shape (n_samples, n_features)\n",
    "    Xbins = np.stack(Xbins, axis=-1)\n",
    "\n",
    "    # Identify the training points that fall into the same bin as the 'point'\n",
    "    same_bin_mask = np.all(Xbins == point_bin, axis=1)\n",
    "\n",
    "    if np.sum(same_bin_mask) == 0:\n",
    "        # If no training point falls in the same bin, one option is to default to the majority class of the training set.\n",
    "        unique, counts = np.unique(ytrain, return_counts=True)\n",
    "        predicted_class = unique[np.argmax(counts)]\n",
    "        return predicted_class\n",
    "    else:\n",
    "        # Use the majority vote from the points falling into the same bin.\n",
    "        subset_labels = ytrain[same_bin_mask]\n",
    "        unique, counts = np.unique(subset_labels, return_counts=True)\n",
    "        predicted_class = unique[np.argmax(counts)]\n",
    "        return predicted_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build an accuracy function (you can use the one you built for the 1st lab). It should output the overall accuracy and the accuracy per class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acc(pred, target):\n",
    "    # accuracy = number of correct predictions / total number of predictions\n",
    "    accuracy = np.count_nonzero(pred == target)  / len(target)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a loss matrix function, the output should clearly express the meaning of the matrix to the user. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_matrix(pred, y, L=None):\n",
    "    \"\"\"\n",
    "    Computes the total loss given predictions and true labels based on a provided loss matrix.\n",
    "\n",
    "    Parameters:\n",
    "      pred : array-like, predicted class labels\n",
    "      y    : array-like, true class labels\n",
    "      L    : 2D ndarray representing the loss matrix.\n",
    "             If None, a default 0/1 loss matrix is used (0 for correct classification, 1 for error).\n",
    "             The loss matrix should be organized such that L[i,j] is the loss incurred for true label i when predicting j.\n",
    "\n",
    "    Returns:\n",
    "      total_loss : the cumulative loss over all predictions.\n",
    "    \"\"\"\n",
    "    pred = np.array(pred)\n",
    "    y = np.array(y)\n",
    "\n",
    "    # obtain the values for each class (in numerical order)\n",
    "    classes = sorted(np.unique(y))\n",
    "\n",
    "    # map each class to an index (0, 1, 2, ...), beacuse the class values can be, for exemple (10, 20, 30 ...)\n",
    "    mapping = {c: i for i, c in enumerate(classes)}\n",
    "\n",
    "    # if no loss matrix is provided, use 0 for correct and 1 for incorrect.\n",
    "    if L is None:\n",
    "        n = len(classes)\n",
    "        L = np.ones((n, n)) - np.eye(n)\n",
    "\n",
    "    total_loss = 0.0\n",
    "    # sum loss for each prediction\n",
    "    for yt, yp in zip(y, pred):\n",
    "        total_loss += L[mapping[yt], mapping[yp]]\n",
    "\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use your functions to predict and evaluate for the DensityEstimationDataset below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gaussian-based predictions: [0 2 0 ... 0 2 2]\n",
      "Histogram-based prediction for the first test point: 0\n",
      "Total loss for Gaussian predictions (using 0/1 loss): 77.0\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# Gaussian-based Prediction\n",
    "# -----------------------------\n",
    "model = GDE(X1_train, y1_train)\n",
    "predictions_gaussian = predG(model, X1_val)\n",
    "print(\"Gaussian-based predictions:\", predictions_gaussian)\n",
    "\n",
    "# -----------------------------\n",
    "# Histogram-based Prediction\n",
    "# -----------------------------\n",
    "# Let's predict the class for the first test point using a histogram with 10 bins per dimension.\n",
    "nbins = 10\n",
    "sample_point = X1_val[0]\n",
    "prediction_hist = predH(X1_train, y1_train, sample_point, nbins)\n",
    "print(\"Histogram-based prediction for the first test point:\", prediction_hist)\n",
    "\n",
    "# -----------------------------\n",
    "# Loss Matrix Example\n",
    "# -----------------------------\n",
    "# Here, we compute the simple classification error (0/1 loss) for our Gaussian predictions.\n",
    "loss = loss_matrix(predictions_gaussian, y1_val)\n",
    "print(\"Total loss for Gaussian predictions (using 0/1 loss):\", loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use your functions to predict and evaluate for the Glass Classification dataset here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Think of proper plots to show your results for each dataset. All plots should contain labels and titles describing the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment with different number of bins for your histogram-based predictor. Plot this experiment to show how the performance behave for the different values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
