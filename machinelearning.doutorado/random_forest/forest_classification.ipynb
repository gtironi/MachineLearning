{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9e04931",
   "metadata": {},
   "source": [
    "## Lab 5\n",
    "In this lab, you will implement the random forest algorithm to build models for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa6737ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sbs\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ed8a13",
   "metadata": {},
   "source": [
    "Import the data in 'car.csv', split it into training, testing and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4cf218",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Carro - Tamanho do conjunto de treinamento: 1382 amostras\n",
      "Dataset Carro - Tamanho do conjunto de validação: 173 amostras\n",
      "Dataset Carro - Tamanho do conjunto de teste: 173 amostras\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_182644/1518064520.py:15: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  car_data[column] = car_data[column].replace(mapping)\n"
     ]
    }
   ],
   "source": [
    "#your code here\n",
    "car_data = pd.read_csv('car.csv', header=None)\n",
    "\n",
    "categorical_mappings = {\n",
    "    0: {'vhigh': 3, 'high': 2, 'med': 1, 'low': 0},       # buying\n",
    "    1: {'vhigh': 3, 'high': 2, 'med': 1, 'low': 0},       # maint\n",
    "    2: {'2': 0, '3': 1, '4': 2, '5more': 3},             # doors\n",
    "    3: {'2': 0, '4': 1, 'more': 2},                      # persons\n",
    "    4: {'small': 0, 'med': 1, 'big': 2},                 # lug_boot\n",
    "    5: {'low': 0, 'med': 1, 'high': 2},                  # safety\n",
    "    6: {'unacc': 0, 'acc': 1, 'good': 2, 'vgood': 3}     # class (target)\n",
    "}\n",
    "\n",
    "for column, mapping in categorical_mappings.items():\n",
    "    car_data[column] = car_data[column].replace(mapping)\n",
    "\n",
    "X_car = car_data.iloc[:, :-1].values\n",
    "y_car = car_data.iloc[:, -1].values\n",
    "\n",
    "X_car_train_full, X_car_temp, y_car_train_full, y_car_temp = train_test_split(X_car, y_car, train_size=0.8, random_state=42)\n",
    "\n",
    "X_car_val, X_car_test, y_car_val, y_car_test = train_test_split(X_car_temp, y_car_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "print(f\"Dataset Carro - Tamanho do conjunto de treinamento: {X_car_train_full.shape[0]} amostras\")\n",
    "print(f\"Dataset Carro - Tamanho do conjunto de validação: {X_car_val.shape[0]} amostras\")\n",
    "print(f\"Dataset Carro - Tamanho do conjunto de teste: {X_car_test.shape[0]} amostras\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eefa4cb2",
   "metadata": {},
   "source": [
    "Built the function you need to train a normal decision tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a29b9137",
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code goes here\n",
    "def bootstrap(X, num_bags=10, random_seed=0):\n",
    "    \"\"\"\n",
    "    Given a dataset and a number of bags, sample the dataset with replacement.\n",
    "    This function returns a list of indices with compatible dimensionality.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : ndarray\n",
    "        A dataset\n",
    "    num_bags : int, default 10\n",
    "        The number of bags to create\n",
    "    random_seed : int, default 0\n",
    "        Seed for the random number generator to ensure reproducibility.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list of ndarray\n",
    "        The list contains `num_bags` integer one-dimensional ndarrays.\n",
    "        Each of these contains the indices corresponding to the sampled datapoints in `X`.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    * The number of datapoints in each bag will match the number of datapoints in the given dataset.\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(random_seed)\n",
    "    num_samples = len(X)\n",
    "    bags = []\n",
    "    for _ in range(num_bags):\n",
    "        # Generate indices with replacement\n",
    "        indices = rng.choice(num_samples, size=num_samples, replace=True)\n",
    "        bags.append(indices)\n",
    "    return bags\n",
    "\n",
    "\n",
    "def aggregate_regression(preds):\n",
    "    \"\"\"\n",
    "    Aggregate predictions by several estimators.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    preds : list of ndarray\n",
    "        Predictions from multiple estimators. All ndarrays in this list should have the same\n",
    "        dimensionality.\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    ndarray\n",
    "        The mean of the predictions.\n",
    "    \"\"\"\n",
    "    # Calculate the mean across all predictions\n",
    "    return np.mean(preds, axis=0)\n",
    "\n",
    "def regression_criterion(region):\n",
    "    \"\"\"\n",
    "    Implements the sum of squared error criterion in a region.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    region : ndarray\n",
    "        Array of shape (N,) containing the values of the target values\n",
    "        for N datapoints in the training set.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        The sum of squared error.\n",
    "\n",
    "    Note\n",
    "    ----\n",
    "    The error for an empty region should be infinity (use: float(\"inf\")).\n",
    "    This avoids creating empty regions.\n",
    "    \"\"\"\n",
    "    if len(region) == 0:\n",
    "        return float(\"inf\")\n",
    "\n",
    "    if isinstance(region, pd.DataFrame) or isinstance(region, pd.Series):\n",
    "        region = region.values\n",
    "\n",
    "    mean_value = np.mean(region)\n",
    "    squared_errors = (region - mean_value) ** 2\n",
    "\n",
    "    return np.sum(squared_errors)\n",
    "\n",
    "\n",
    "def split_region(region_data, feature_index, tau):\n",
    "    \"\"\"\n",
    "    Given a region, splits it based on the feature indicated by\n",
    "    `feature_index`, the region will be split in two, where\n",
    "    one side will contain all points with the feature with values\n",
    "    lower than `tau`, and the other split will contain the\n",
    "    remaining datapoints.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    region_data : array of size (n_samples, n_features + 1)\n",
    "        a partition of the dataset (or the full dataset) to be split,\n",
    "        including the target variable as the last column.\n",
    "    feature_index : int\n",
    "        the index of the feature (column of the region array) used to make this partition\n",
    "    tau : float\n",
    "        The threshold used to make this partition\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    left_partition : array\n",
    "        datapoints in `region_data` where feature <= `tau`\n",
    "    right_partition : array\n",
    "        datapoints in `region_data` where feature > `tau`\n",
    "    \"\"\"\n",
    "    if isinstance(region_data, pd.DataFrame):\n",
    "        feature_values = region_data.iloc[:, feature_index].values\n",
    "    else:\n",
    "        feature_values = region_data[:, feature_index]\n",
    "\n",
    "    left_indices = np.where(feature_values <= tau)[0]\n",
    "    right_indices = np.where(feature_values > tau)[0]\n",
    "\n",
    "    left_partition = region_data[left_indices]\n",
    "    right_partition = region_data[right_indices]\n",
    "\n",
    "    return left_partition, right_partition\n",
    "\n",
    "\n",
    "class TreeNode:\n",
    "    \"\"\"\n",
    "    Represents a node in the Decision Tree.\n",
    "    \"\"\"\n",
    "    def __init__(self, data, feature_idx, feature_val, sq_error, is_leaf=False):\n",
    "        self.data = data\n",
    "        self.feature_idx = feature_idx\n",
    "        self.feature_val = feature_val\n",
    "        self.sq_error = sq_error\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "        self._is_leaf = is_leaf\n",
    "\n",
    "    @property\n",
    "    def is_leaf(self):\n",
    "        \"\"\"\n",
    "        Determines if the node is a leaf node.\n",
    "        A node is a leaf if it was explicitly marked as such, or if it has no children.\n",
    "        \"\"\"\n",
    "        return self._is_leaf or (self.left is None and self.right is None)\n",
    "\n",
    "\n",
    "class DecisionTree:\n",
    "    \"\"\"\n",
    "    Custom Decision Tree Regressor implementation.\n",
    "    \"\"\"\n",
    "    def __init__(self, criterion=regression_criterion, max_depth=None, min_samples_leaf=1, num_features=None, random_state=None):\n",
    "        \"\"\"\n",
    "        Initializes the DecisionTree Regressor.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        criterion : function, default regression_criterion\n",
    "            The function used to evaluate the quality of a split (e.g., sum of squared errors).\n",
    "        max_depth : int, default None\n",
    "            The maximum depth of the tree. If None, then nodes are expanded until\n",
    "            all leaves are pure or until all leaves contain less than min_samples_leaf samples.\n",
    "        min_samples_leaf : int, default 1\n",
    "            The minimum number of samples required to be at a leaf node.\n",
    "        num_features : int or float or str, default None\n",
    "            The number of features to consider when looking for the best split:\n",
    "            - If int, then consider `num_features` features at each split.\n",
    "            - If float, then `num_features` is a fraction and `int(num_features * n_features)` features are considered.\n",
    "            - If \"auto\", then `num_features=sqrt(n_features)`.\n",
    "            - If \"sqrt\", then `num_features=sqrt(n_features)`.\n",
    "            - If \"log2\", then `num_features=log2(n_features)`.\n",
    "            - If None, then all features are considered.\n",
    "        random_state : int, default None\n",
    "            Controls the random seed for reproducibility of feature selection within the tree.\n",
    "        \"\"\"\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.criterion = criterion\n",
    "        self.tree = None\n",
    "        self.num_features = num_features\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        \"\"\"\n",
    "        Builds the decision tree from the training data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X_train : ndarray\n",
    "            Training features.\n",
    "        y_train : ndarray\n",
    "            Training target values.\n",
    "        \"\"\"\n",
    "        train_data = np.column_stack((X_train, y_train))\n",
    "        self.tree = self.create_tree(train_data, current_depth=0)\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        \"\"\"\n",
    "        Predicts target values for new data using the trained Decision Tree.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X_test : ndarray\n",
    "            Features of the new data points.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        ndarray\n",
    "            Predicted target values.\n",
    "        \"\"\"\n",
    "        if self.tree is None:\n",
    "            raise RuntimeError(\"Decision Tree not fitted. Call fit() before predict().\")\n",
    "\n",
    "        predictions = np.array([self.predict_sample(x, self.tree) for x in X_test])\n",
    "        return predictions\n",
    "\n",
    "    def create_tree(self, data, current_depth):\n",
    "        \"\"\"\n",
    "        Recursively creates the decision tree.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        data : ndarray\n",
    "            The subset of data for the current node (features and target).\n",
    "        current_depth : int\n",
    "            The current depth of the node in the tree.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        TreeNode\n",
    "            The created node (either internal or leaf).\n",
    "        \"\"\"\n",
    "        # Calculate sum of squared error for the current node's data\n",
    "        current_node_sq_error = self.criterion(data[:, -1])\n",
    "\n",
    "        # Stopping criteria:\n",
    "        # 1. Max depth reached\n",
    "        if self.max_depth is not None and current_depth >= self.max_depth:\n",
    "            return TreeNode(data, None, None, current_node_sq_error, is_leaf=True)\n",
    "\n",
    "        # 2. Minimum samples per leaf reached\n",
    "        if len(data) <= self.min_samples_leaf:\n",
    "            return TreeNode(data, None, None, current_node_sq_error, is_leaf=True)\n",
    "\n",
    "        # Find the best split for the current data\n",
    "        split_data_tuple, split_feature_idx, split_feature_val, best_split_sq_error = self.find_bestsplit(data)\n",
    "\n",
    "        # 3. No valid split found or no improvement\n",
    "        if split_feature_idx is None:\n",
    "            return TreeNode(data, None, None, current_node_sq_error, is_leaf=True)\n",
    "\n",
    "        # Create the current node\n",
    "        node = TreeNode(data, split_feature_idx, split_feature_val, current_node_sq_error)\n",
    "\n",
    "        # 4. Check if splits result in regions smaller than min_samples_leaf\n",
    "        # This check ensures that even if a split is found, if it leads to\n",
    "        # children nodes that are too small, we make the current node a leaf.\n",
    "        left_data, right_data = split_data_tuple\n",
    "        if len(left_data) < self.min_samples_leaf or len(right_data) < self.min_samples_leaf:\n",
    "             return TreeNode(data, None, None, current_node_sq_error, is_leaf=True)\n",
    "\n",
    "        # Recursively build left and right children\n",
    "        node.left = self.create_tree(left_data, current_depth + 1)\n",
    "        node.right = self.create_tree(right_data, current_depth + 1)\n",
    "\n",
    "        return node\n",
    "\n",
    "    def find_bestsplit(self, data):\n",
    "        \"\"\"\n",
    "        Finds the best feature and threshold to split the given data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        data : ndarray\n",
    "            The subset of data for which to find the best split.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        tuple\n",
    "            (regions, best_feature_index, best_tau, current_node_sq_error)\n",
    "            regions : tuple of ndarrays (left_partition, right_partition)\n",
    "            best_feature_index : int or None\n",
    "            best_tau : float or None\n",
    "            current_node_sq_error : float (sum of squared error for the current node's data)\n",
    "        \"\"\"\n",
    "        if isinstance(data, pd.DataFrame):\n",
    "            data = data.values\n",
    "        if isinstance(data, pd.Series):\n",
    "            data = data.values\n",
    "\n",
    "        best_sq_error = float('inf')\n",
    "        best_feature_index = None\n",
    "        best_tau = None\n",
    "        regions = None\n",
    "\n",
    "        n_samples, n_total_cols = data.shape\n",
    "        n_features = n_total_cols - 1  # Exclude target column (last column)\n",
    "\n",
    "        # Calculate sum of squared error for the current node's data before splitting\n",
    "        current_node_sq_error = self.criterion(data[:, -1])\n",
    "\n",
    "        # Determine the actual number of features to sample for this split\n",
    "        num_features_for_split = n_features  # Default to all features if num_features is None or invalid\n",
    "        if self.num_features is not None:\n",
    "            if isinstance(self.num_features, int):\n",
    "                num_features_for_split = min(self.num_features, n_features)\n",
    "            elif isinstance(self.num_features, float):\n",
    "                num_features_for_split = int(self.num_features * n_features)\n",
    "            elif self.num_features == \"auto\" or self.num_features == \"sqrt\":\n",
    "                num_features_for_split = int(np.sqrt(n_features))\n",
    "            elif self.num_features == \"log2\":\n",
    "                num_features_for_split = int(np.log2(n_features))\n",
    "\n",
    "        # Handle case where num_features_for_split might be 0 or negative\n",
    "        if num_features_for_split <= 0:\n",
    "            return None, None, None, current_node_sq_error # No valid split possible\n",
    "\n",
    "        # Randomly select a subset of features for this split\n",
    "        rng_split = np.random.default_rng(self.random_state)\n",
    "        all_feature_indices = np.arange(n_features)\n",
    "        selected_feature_indices = rng_split.choice(all_feature_indices, size=num_features_for_split, replace=False)\n",
    "\n",
    "        for feature_index in selected_feature_indices:  # Iterate only over selected features\n",
    "            feature_values = data[:, feature_index]\n",
    "            possible_taus = np.unique(feature_values)\n",
    "\n",
    "            # Avoid splitting on features with only one unique value (no meaningful split)\n",
    "            if len(possible_taus) < 2:\n",
    "                continue\n",
    "\n",
    "            for tau in possible_taus:\n",
    "                left, right = split_region(data, feature_index, tau) # Use the global split_region function\n",
    "\n",
    "                # Ensure splits are not empty before evaluating criterion\n",
    "                if len(left) == 0 or len(right) == 0:\n",
    "                    continue  # Skip if split creates empty regions\n",
    "\n",
    "                sq_error = self.criterion(left[:, -1]) + self.criterion(right[:, -1])\n",
    "\n",
    "                if sq_error < best_sq_error:\n",
    "                    best_sq_error = sq_error\n",
    "                    best_feature_index = feature_index\n",
    "                    best_tau = tau\n",
    "                    regions = (left, right)\n",
    "\n",
    "        # If no split was found that improves the criterion (best_sq_error remains inf)\n",
    "        # or if the best split found is not better than the current node's error (no gain)\n",
    "        if best_feature_index is None or best_sq_error >= current_node_sq_error:\n",
    "            return None, None, None, current_node_sq_error  # Indicate no valid split found\n",
    "\n",
    "        return regions, best_feature_index, best_tau, current_node_sq_error\n",
    "\n",
    "    def predict_sample(self, sample, node):\n",
    "        \"\"\"\n",
    "        Recursively predicts the target value for a single sample.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        sample : ndarray\n",
    "            A single data point (features only).\n",
    "        node : TreeNode\n",
    "            The current node in the decision tree.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "            The predicted target value for the sample.\n",
    "        \"\"\"\n",
    "        if node.is_leaf:\n",
    "            # For a leaf node, the prediction is the mean of the target values in that node's data\n",
    "            return np.mean(node.data[:, -1])\n",
    "\n",
    "        # Traverse the tree based on the feature and threshold\n",
    "        if sample[node.feature_idx] <= node.feature_val: # Changed to <= as per split_region\n",
    "            if node.left is None: # Handle cases where a branch might be None (e.g., due to max_depth or min_samples_leaf)\n",
    "                return np.mean(node.data[:, -1]) # Return current node's mean if child is None\n",
    "            return self.predict_sample(sample, node.left)\n",
    "        else:\n",
    "            if node.right is None: # Handle cases where a branch might be None\n",
    "                return np.mean(node.data[:, -1])\n",
    "            return self.predict_sample(sample, node.right)\n",
    "\n",
    "    def print_tree(self):\n",
    "        \"\"\"Prints the structure of the trained decision tree.\"\"\"\n",
    "        self._print_tree(self.tree)\n",
    "\n",
    "    def _print_tree(self, node, depth=0, prefix=\"\"):\n",
    "        \"\"\"Helper function for recursively printing the tree.\"\"\"\n",
    "        if node is None:\n",
    "            return\n",
    "        indent = \"  \" * depth\n",
    "        if node.is_leaf:\n",
    "            print(f\"{indent}{prefix}Leaf: Predict {np.mean(node.data[:, -1]):.2f}, Samples: {len(node.data)}, SSE = {node.sq_error:.2f}\")\n",
    "        else:\n",
    "            print(f\"{indent}{prefix}Node: Feature {node.feature_idx}, Threshold {node.feature_val:.2f}, SSE = {node.sq_error:.2f}\")\n",
    "            self._print_tree(node.left, depth + 1, prefix=\"L--> \")\n",
    "            self._print_tree(node.right, depth + 1, prefix=\"R--> \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3dcd499",
   "metadata": {},
   "source": [
    "Buit a function that train a random forest and predicts the class of a new point given your trained random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16520f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code goes here\n",
    "class RandomForest:\n",
    "    \"\"\"\n",
    "    Custom Random Forest Regressor implementation.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_features=None, min_samples_leaf=1, max_depth=None, num_estimators=10, random_state=None):\n",
    "        \"\"\"\n",
    "        Initializes the RandomForest Regressor.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        num_features : int or float or str, default None\n",
    "            The number of features to consider when looking for the best split in each tree.\n",
    "            Passed to the underlying DecisionTreeRegressor.\n",
    "        min_samples_leaf : int, default 1\n",
    "            The minimum number of samples required to be at a leaf node for each tree.\n",
    "        max_depth : int, default None\n",
    "            The maximum depth of each decision tree in the forest.\n",
    "        num_estimators : int, default 10\n",
    "            The number of decision trees (estimators) in the forest.\n",
    "        random_state : int, default None\n",
    "            Controls the random seed for reproducibility of the overall Random Forest.\n",
    "            Used for bootstrapping and for passing unique seeds to individual trees.\n",
    "        \"\"\"\n",
    "        self.num_features = num_features\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.max_depth = max_depth\n",
    "        self.num_estimators = num_estimators\n",
    "        self.random_state = random_state\n",
    "        self.estimators = []  # List to store trained DecisionTree models\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        \"\"\"\n",
    "        Trains the Random Forest model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X_train : ndarray\n",
    "            Training features.\n",
    "        y_train : ndarray\n",
    "            Training target values.\n",
    "        \"\"\"\n",
    "        self.estimators = []  # Clear any existing estimators\n",
    "        rng = np.random.default_rng(self.random_state)  # Random number generator for reproducibility\n",
    "\n",
    "        # Generate all bags of indices upfront\n",
    "        all_bag_indices = bootstrap(X_train, num_bags=self.num_estimators, random_seed=self.random_state)\n",
    "\n",
    "        for i in range(self.num_estimators):\n",
    "            # Get the data for the current bag\n",
    "            bag_indices = all_bag_indices[i]\n",
    "            bag_X = X_train[bag_indices]\n",
    "            bag_y = y_train[bag_indices]\n",
    "\n",
    "            # Build and train a custom DecisionTree for the current bag\n",
    "            # Pass a unique random state to each tree for its internal feature selection\n",
    "            tree_random_state = rng.integers(0, 1000000) # Use a large range for seeds\n",
    "            rf_estimator = DecisionTree(\n",
    "                max_depth=self.max_depth,\n",
    "                min_samples_leaf=self.min_samples_leaf,\n",
    "                num_features=self.num_features,\n",
    "                random_state=tree_random_state\n",
    "            )\n",
    "            rf_estimator.fit(bag_X, bag_y)\n",
    "            self.estimators.append(rf_estimator)\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        \"\"\"\n",
    "        Predicts target values for new data using the trained Random Forest.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X_test : ndarray\n",
    "            Features of the new data points.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        ndarray\n",
    "            Predicted target values.\n",
    "        \"\"\"\n",
    "        if not self.estimators:\n",
    "            raise RuntimeError(\"Random Forest not fitted. Call fit() before predict().\")\n",
    "\n",
    "        predictions = []\n",
    "        for estimator in self.estimators:\n",
    "            # Get predictions from each individual tree\n",
    "            pred = estimator.predict(X_test)\n",
    "            predictions.append(pred)\n",
    "        # Aggregate predictions using the mean\n",
    "        return aggregate_regression(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f61b8faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_score(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calcula a acurácia da classificação.\n",
    "    \"\"\"\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_pred = np.asarray(y_pred)\n",
    "    return np.mean(y_true == y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd6240a",
   "metadata": {},
   "source": [
    "Builld functions to evaluate your predictions, use this functions to find optimal hyperparameters using also our validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de8d5586",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testando Carro: n_est=10, max_depth=5, min_leaf=1, n_feat=0.5\n",
      "  Acurácia de Validação Carro: 0.0289\n",
      "Testando Carro: n_est=10, max_depth=5, min_leaf=1, n_feat=0.7\n",
      "  Acurácia de Validação Carro: 0.1098\n",
      "Testando Carro: n_est=10, max_depth=5, min_leaf=1, n_feat=1.0\n",
      "  Acurácia de Validação Carro: 0.5318\n",
      "Testando Carro: n_est=10, max_depth=5, min_leaf=5, n_feat=0.5\n",
      "  Acurácia de Validação Carro: 0.0289\n",
      "Testando Carro: n_est=10, max_depth=5, min_leaf=5, n_feat=0.7\n",
      "  Acurácia de Validação Carro: 0.1098\n",
      "Testando Carro: n_est=10, max_depth=5, min_leaf=5, n_feat=1.0\n",
      "  Acurácia de Validação Carro: 0.5318\n",
      "Testando Carro: n_est=10, max_depth=5, min_leaf=10, n_feat=0.5\n",
      "  Acurácia de Validação Carro: 0.0289\n",
      "Testando Carro: n_est=10, max_depth=5, min_leaf=10, n_feat=0.7\n",
      "  Acurácia de Validação Carro: 0.1098\n",
      "Testando Carro: n_est=10, max_depth=5, min_leaf=10, n_feat=1.0\n",
      "  Acurácia de Validação Carro: 0.5318\n",
      "Testando Carro: n_est=10, max_depth=10, min_leaf=1, n_feat=0.5\n",
      "  Acurácia de Validação Carro: 0.0289\n",
      "Testando Carro: n_est=10, max_depth=10, min_leaf=1, n_feat=0.7\n",
      "  Acurácia de Validação Carro: 0.1156\n",
      "Testando Carro: n_est=10, max_depth=10, min_leaf=1, n_feat=1.0\n",
      "  Acurácia de Validação Carro: 0.8786\n",
      "Testando Carro: n_est=10, max_depth=10, min_leaf=5, n_feat=0.5\n",
      "  Acurácia de Validação Carro: 0.0289\n",
      "Testando Carro: n_est=10, max_depth=10, min_leaf=5, n_feat=0.7\n",
      "  Acurácia de Validação Carro: 0.1156\n",
      "Testando Carro: n_est=10, max_depth=10, min_leaf=5, n_feat=1.0\n",
      "  Acurácia de Validação Carro: 0.6936\n",
      "Testando Carro: n_est=10, max_depth=10, min_leaf=10, n_feat=0.5\n",
      "  Acurácia de Validação Carro: 0.0289\n",
      "Testando Carro: n_est=10, max_depth=10, min_leaf=10, n_feat=0.7\n",
      "  Acurácia de Validação Carro: 0.1098\n",
      "Testando Carro: n_est=10, max_depth=10, min_leaf=10, n_feat=1.0\n",
      "  Acurácia de Validação Carro: 0.6185\n",
      "Testando Carro: n_est=10, max_depth=15, min_leaf=1, n_feat=0.5\n",
      "  Acurácia de Validação Carro: 0.0289\n",
      "Testando Carro: n_est=10, max_depth=15, min_leaf=1, n_feat=0.7\n",
      "  Acurácia de Validação Carro: 0.1156\n",
      "Testando Carro: n_est=10, max_depth=15, min_leaf=1, n_feat=1.0\n",
      "  Acurácia de Validação Carro: 0.8902\n",
      "Testando Carro: n_est=10, max_depth=15, min_leaf=5, n_feat=0.5\n",
      "  Acurácia de Validação Carro: 0.0289\n",
      "Testando Carro: n_est=10, max_depth=15, min_leaf=5, n_feat=0.7\n",
      "  Acurácia de Validação Carro: 0.1156\n",
      "Testando Carro: n_est=10, max_depth=15, min_leaf=5, n_feat=1.0\n",
      "  Acurácia de Validação Carro: 0.6936\n",
      "Testando Carro: n_est=10, max_depth=15, min_leaf=10, n_feat=0.5\n",
      "  Acurácia de Validação Carro: 0.0289\n",
      "Testando Carro: n_est=10, max_depth=15, min_leaf=10, n_feat=0.7\n",
      "  Acurácia de Validação Carro: 0.1098\n",
      "Testando Carro: n_est=10, max_depth=15, min_leaf=10, n_feat=1.0\n",
      "  Acurácia de Validação Carro: 0.6185\n",
      "Testando Carro: n_est=10, max_depth=None, min_leaf=1, n_feat=0.5\n",
      "  Acurácia de Validação Carro: 0.0289\n",
      "Testando Carro: n_est=10, max_depth=None, min_leaf=1, n_feat=0.7\n",
      "  Acurácia de Validação Carro: 0.1156\n",
      "Testando Carro: n_est=10, max_depth=None, min_leaf=1, n_feat=1.0\n",
      "  Acurácia de Validação Carro: 0.8902\n",
      "Testando Carro: n_est=10, max_depth=None, min_leaf=5, n_feat=0.5\n",
      "  Acurácia de Validação Carro: 0.0289\n",
      "Testando Carro: n_est=10, max_depth=None, min_leaf=5, n_feat=0.7\n",
      "  Acurácia de Validação Carro: 0.1156\n",
      "Testando Carro: n_est=10, max_depth=None, min_leaf=5, n_feat=1.0\n",
      "  Acurácia de Validação Carro: 0.6936\n",
      "Testando Carro: n_est=10, max_depth=None, min_leaf=10, n_feat=0.5\n",
      "  Acurácia de Validação Carro: 0.0289\n",
      "Testando Carro: n_est=10, max_depth=None, min_leaf=10, n_feat=0.7\n",
      "  Acurácia de Validação Carro: 0.1098\n",
      "Testando Carro: n_est=10, max_depth=None, min_leaf=10, n_feat=1.0\n",
      "  Acurácia de Validação Carro: 0.6185\n",
      "Testando Carro: n_est=20, max_depth=5, min_leaf=1, n_feat=0.5\n",
      "  Acurácia de Validação Carro: 0.0000\n",
      "Testando Carro: n_est=20, max_depth=5, min_leaf=1, n_feat=0.7\n",
      "  Acurácia de Validação Carro: 0.1098\n",
      "Testando Carro: n_est=20, max_depth=5, min_leaf=1, n_feat=1.0\n",
      "  Acurácia de Validação Carro: 0.5318\n",
      "Testando Carro: n_est=20, max_depth=5, min_leaf=5, n_feat=0.5\n",
      "  Acurácia de Validação Carro: 0.0000\n",
      "Testando Carro: n_est=20, max_depth=5, min_leaf=5, n_feat=0.7\n",
      "  Acurácia de Validação Carro: 0.1098\n",
      "Testando Carro: n_est=20, max_depth=5, min_leaf=5, n_feat=1.0\n",
      "  Acurácia de Validação Carro: 0.5318\n",
      "Testando Carro: n_est=20, max_depth=5, min_leaf=10, n_feat=0.5\n",
      "  Acurácia de Validação Carro: 0.0000\n",
      "Testando Carro: n_est=20, max_depth=5, min_leaf=10, n_feat=0.7\n",
      "  Acurácia de Validação Carro: 0.1098\n",
      "Testando Carro: n_est=20, max_depth=5, min_leaf=10, n_feat=1.0\n",
      "  Acurácia de Validação Carro: 0.5318\n",
      "Testando Carro: n_est=20, max_depth=10, min_leaf=1, n_feat=0.5\n",
      "  Acurácia de Validação Carro: 0.0000\n",
      "Testando Carro: n_est=20, max_depth=10, min_leaf=1, n_feat=0.7\n",
      "  Acurácia de Validação Carro: 0.1098\n",
      "Testando Carro: n_est=20, max_depth=10, min_leaf=1, n_feat=1.0\n",
      "  Acurácia de Validação Carro: 0.8728\n",
      "Testando Carro: n_est=20, max_depth=10, min_leaf=5, n_feat=0.5\n",
      "  Acurácia de Validação Carro: 0.0000\n",
      "Testando Carro: n_est=20, max_depth=10, min_leaf=5, n_feat=0.7\n",
      "  Acurácia de Validação Carro: 0.1098\n",
      "Testando Carro: n_est=20, max_depth=10, min_leaf=5, n_feat=1.0\n",
      "  Acurácia de Validação Carro: 0.6301\n",
      "Testando Carro: n_est=20, max_depth=10, min_leaf=10, n_feat=0.5\n",
      "  Acurácia de Validação Carro: 0.0000\n",
      "Testando Carro: n_est=20, max_depth=10, min_leaf=10, n_feat=0.7\n",
      "  Acurácia de Validação Carro: 0.1098\n",
      "Testando Carro: n_est=20, max_depth=10, min_leaf=10, n_feat=1.0\n",
      "  Acurácia de Validação Carro: 0.6185\n",
      "Testando Carro: n_est=20, max_depth=15, min_leaf=1, n_feat=0.5\n",
      "  Acurácia de Validação Carro: 0.0000\n",
      "Testando Carro: n_est=20, max_depth=15, min_leaf=1, n_feat=0.7\n",
      "  Acurácia de Validação Carro: 0.1098\n",
      "Testando Carro: n_est=20, max_depth=15, min_leaf=1, n_feat=1.0\n",
      "  Acurácia de Validação Carro: 0.8786\n",
      "Testando Carro: n_est=20, max_depth=15, min_leaf=5, n_feat=0.5\n",
      "  Acurácia de Validação Carro: 0.0000\n",
      "Testando Carro: n_est=20, max_depth=15, min_leaf=5, n_feat=0.7\n",
      "  Acurácia de Validação Carro: 0.1098\n",
      "Testando Carro: n_est=20, max_depth=15, min_leaf=5, n_feat=1.0\n",
      "  Acurácia de Validação Carro: 0.6301\n",
      "Testando Carro: n_est=20, max_depth=15, min_leaf=10, n_feat=0.5\n",
      "  Acurácia de Validação Carro: 0.0000\n",
      "Testando Carro: n_est=20, max_depth=15, min_leaf=10, n_feat=0.7\n",
      "  Acurácia de Validação Carro: 0.1098\n",
      "Testando Carro: n_est=20, max_depth=15, min_leaf=10, n_feat=1.0\n",
      "  Acurácia de Validação Carro: 0.6185\n",
      "Testando Carro: n_est=20, max_depth=None, min_leaf=1, n_feat=0.5\n",
      "  Acurácia de Validação Carro: 0.0000\n",
      "Testando Carro: n_est=20, max_depth=None, min_leaf=1, n_feat=0.7\n",
      "  Acurácia de Validação Carro: 0.1098\n",
      "Testando Carro: n_est=20, max_depth=None, min_leaf=1, n_feat=1.0\n",
      "  Acurácia de Validação Carro: 0.8786\n",
      "Testando Carro: n_est=20, max_depth=None, min_leaf=5, n_feat=0.5\n",
      "  Acurácia de Validação Carro: 0.0000\n",
      "Testando Carro: n_est=20, max_depth=None, min_leaf=5, n_feat=0.7\n",
      "  Acurácia de Validação Carro: 0.1098\n",
      "Testando Carro: n_est=20, max_depth=None, min_leaf=5, n_feat=1.0\n",
      "  Acurácia de Validação Carro: 0.6301\n",
      "Testando Carro: n_est=20, max_depth=None, min_leaf=10, n_feat=0.5\n",
      "  Acurácia de Validação Carro: 0.0000\n",
      "Testando Carro: n_est=20, max_depth=None, min_leaf=10, n_feat=0.7\n",
      "  Acurácia de Validação Carro: 0.1098\n",
      "Testando Carro: n_est=20, max_depth=None, min_leaf=10, n_feat=1.0\n",
      "  Acurácia de Validação Carro: 0.6185\n",
      "Testando Carro: n_est=50, max_depth=5, min_leaf=1, n_feat=0.5\n",
      "  Acurácia de Validação Carro: 0.0000\n",
      "Testando Carro: n_est=50, max_depth=5, min_leaf=1, n_feat=0.7\n",
      "  Acurácia de Validação Carro: 0.0116\n",
      "Testando Carro: n_est=50, max_depth=5, min_leaf=1, n_feat=1.0\n",
      "  Acurácia de Validação Carro: 0.5260\n",
      "Testando Carro: n_est=50, max_depth=5, min_leaf=5, n_feat=0.5\n",
      "  Acurácia de Validação Carro: 0.0000\n",
      "Testando Carro: n_est=50, max_depth=5, min_leaf=5, n_feat=0.7\n",
      "  Acurácia de Validação Carro: 0.0116\n",
      "Testando Carro: n_est=50, max_depth=5, min_leaf=5, n_feat=1.0\n",
      "  Acurácia de Validação Carro: 0.5260\n",
      "Testando Carro: n_est=50, max_depth=5, min_leaf=10, n_feat=0.5\n",
      "  Acurácia de Validação Carro: 0.0000\n",
      "Testando Carro: n_est=50, max_depth=5, min_leaf=10, n_feat=0.7\n",
      "  Acurácia de Validação Carro: 0.0116\n",
      "Testando Carro: n_est=50, max_depth=5, min_leaf=10, n_feat=1.0\n",
      "  Acurácia de Validação Carro: 0.5260\n",
      "Testando Carro: n_est=50, max_depth=10, min_leaf=1, n_feat=0.5\n",
      "  Acurácia de Validação Carro: 0.0000\n",
      "Testando Carro: n_est=50, max_depth=10, min_leaf=1, n_feat=0.7\n",
      "  Acurácia de Validação Carro: 0.0347\n",
      "Testando Carro: n_est=50, max_depth=10, min_leaf=1, n_feat=1.0\n",
      "  Acurácia de Validação Carro: 0.8324\n",
      "Testando Carro: n_est=50, max_depth=10, min_leaf=5, n_feat=0.5\n",
      "  Acurácia de Validação Carro: 0.0000\n",
      "Testando Carro: n_est=50, max_depth=10, min_leaf=5, n_feat=0.7\n",
      "  Acurácia de Validação Carro: 0.0289\n",
      "Testando Carro: n_est=50, max_depth=10, min_leaf=5, n_feat=1.0\n",
      "  Acurácia de Validação Carro: 0.6185\n",
      "Testando Carro: n_est=50, max_depth=10, min_leaf=10, n_feat=0.5\n",
      "  Acurácia de Validação Carro: 0.0000\n",
      "Testando Carro: n_est=50, max_depth=10, min_leaf=10, n_feat=0.7\n",
      "  Acurácia de Validação Carro: 0.0289\n",
      "Testando Carro: n_est=50, max_depth=10, min_leaf=10, n_feat=1.0\n",
      "  Acurácia de Validação Carro: 0.5780\n",
      "Testando Carro: n_est=50, max_depth=15, min_leaf=1, n_feat=0.5\n",
      "  Acurácia de Validação Carro: 0.0000\n",
      "Testando Carro: n_est=50, max_depth=15, min_leaf=1, n_feat=0.7\n",
      "  Acurácia de Validação Carro: 0.0347\n",
      "Testando Carro: n_est=50, max_depth=15, min_leaf=1, n_feat=1.0\n",
      "  Acurácia de Validação Carro: 0.8497\n",
      "Testando Carro: n_est=50, max_depth=15, min_leaf=5, n_feat=0.5\n",
      "  Acurácia de Validação Carro: 0.0000\n",
      "Testando Carro: n_est=50, max_depth=15, min_leaf=5, n_feat=0.7\n",
      "  Acurácia de Validação Carro: 0.0289\n",
      "Testando Carro: n_est=50, max_depth=15, min_leaf=5, n_feat=1.0\n",
      "  Acurácia de Validação Carro: 0.6185\n",
      "Testando Carro: n_est=50, max_depth=15, min_leaf=10, n_feat=0.5\n",
      "  Acurácia de Validação Carro: 0.0000\n",
      "Testando Carro: n_est=50, max_depth=15, min_leaf=10, n_feat=0.7\n",
      "  Acurácia de Validação Carro: 0.0289\n",
      "Testando Carro: n_est=50, max_depth=15, min_leaf=10, n_feat=1.0\n",
      "  Acurácia de Validação Carro: 0.5780\n",
      "Testando Carro: n_est=50, max_depth=None, min_leaf=1, n_feat=0.5\n",
      "  Acurácia de Validação Carro: 0.0000\n",
      "Testando Carro: n_est=50, max_depth=None, min_leaf=1, n_feat=0.7\n",
      "  Acurácia de Validação Carro: 0.0347\n",
      "Testando Carro: n_est=50, max_depth=None, min_leaf=1, n_feat=1.0\n",
      "  Acurácia de Validação Carro: 0.8497\n",
      "Testando Carro: n_est=50, max_depth=None, min_leaf=5, n_feat=0.5\n",
      "  Acurácia de Validação Carro: 0.0000\n",
      "Testando Carro: n_est=50, max_depth=None, min_leaf=5, n_feat=0.7\n",
      "  Acurácia de Validação Carro: 0.0289\n",
      "Testando Carro: n_est=50, max_depth=None, min_leaf=5, n_feat=1.0\n",
      "  Acurácia de Validação Carro: 0.6185\n",
      "Testando Carro: n_est=50, max_depth=None, min_leaf=10, n_feat=0.5\n",
      "  Acurácia de Validação Carro: 0.0000\n",
      "Testando Carro: n_est=50, max_depth=None, min_leaf=10, n_feat=0.7\n",
      "  Acurácia de Validação Carro: 0.0289\n",
      "Testando Carro: n_est=50, max_depth=None, min_leaf=10, n_feat=1.0\n",
      "  Acurácia de Validação Carro: 0.5780\n",
      "\n",
      "Hiperparâmetros Ótimos para o Dataset Carro: {'num_estimators': 10, 'max_depth': 15, 'min_samples_leaf': 1, 'num_features': 1.0}\n",
      "Melhor Acurácia de Validação para o Dataset Carro: 0.8902\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'num_estimators': [10, 20, 50],\n",
    "    'max_depth': [5, 10, 15, None],\n",
    "    'min_samples_leaf': [1, 5, 10],\n",
    "    'num_features': [0.5, 0.7, 1.0] # Fração de features a considerar\n",
    "}\n",
    "\n",
    "best_accuracy_car = 0.0 # Inicializa com 0 para maximizar\n",
    "best_params_car = {}\n",
    "\n",
    "# Itera sobre todas as combinações de hiperparâmetros\n",
    "for n_est in param_grid['num_estimators']:\n",
    "    for m_depth in param_grid['max_depth']:\n",
    "        for min_leaf in param_grid['min_samples_leaf']:\n",
    "            for n_feat in param_grid['num_features']:\n",
    "                print(f\"Testando Carro: n_est={n_est}, max_depth={m_depth}, min_leaf={min_leaf}, n_feat={n_feat}\")\n",
    "                rf_model_car = RandomForest(\n",
    "                    num_estimators=n_est,\n",
    "                    max_depth=m_depth,\n",
    "                    min_samples_leaf=min_leaf,\n",
    "                    num_features=n_feat,\n",
    "                    random_state=42\n",
    "                )\n",
    "                rf_model_car.fit(X_car_train_full, y_car_train_full)\n",
    "                y_car_pred_val = rf_model_car.predict(X_car_val)\n",
    "                current_accuracy_car = accuracy_score(y_car_val, y_car_pred_val)\n",
    "\n",
    "                if current_accuracy_car > best_accuracy_car: # Maximizar acurácia\n",
    "                    best_accuracy_car = current_accuracy_car\n",
    "                    best_params_car = {\n",
    "                        'num_estimators': n_est,\n",
    "                        'max_depth': m_depth,\n",
    "                        'min_samples_leaf': min_leaf,\n",
    "                        'num_features': n_feat\n",
    "                    }\n",
    "                print(f\"  Acurácia de Validação Carro: {current_accuracy_car:.4f}\")\n",
    "\n",
    "print(f\"\\nHiperparâmetros Ótimos para o Dataset Carro: {best_params_car}\")\n",
    "print(f\"Melhor Acurácia de Validação para o Dataset Carro: {best_accuracy_car:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cdc2e42",
   "metadata": {},
   "source": [
    "Train a final random forest using your optimal hyperparameters and both the training and validation sets. Predict for the datapoints in the testing set and evaluate your final predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4d1c32b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Carro - Tamanho do conjunto Combinado Treino + Validação: 1555 amostras\n",
      "\n",
      "Treinando o modelo final para o Dataset Carro...\n",
      "Acurácia Final do Teste para o Dataset Carro: 0.9017\n"
     ]
    }
   ],
   "source": [
    "X_car_train_val = np.vstack((X_car_train_full, X_car_val))\n",
    "y_car_train_val = np.concatenate((y_car_train_full, y_car_val))\n",
    "\n",
    "print(f\"Dataset Carro - Tamanho do conjunto Combinado Treino + Validação: {X_car_train_val.shape[0]} amostras\")\n",
    "\n",
    "# Treinar o modelo Random Forest final com hiperparâmetros ótimos\n",
    "print(\"\\nTreinando o modelo final para o Dataset Carro...\")\n",
    "final_rf_car = RandomForest(random_state=42, **best_params_car)\n",
    "final_rf_car.fit(X_car_train_val, y_car_train_val)\n",
    "\n",
    "# Prever no conjunto de teste\n",
    "y_car_pred_test = final_rf_car.predict(X_car_test)\n",
    "final_rmse_car = accuracy_score(y_car_test, y_car_pred_test)\n",
    "print(f\"Acurácia Final do Teste para o Dataset Carro: {final_rmse_car:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
