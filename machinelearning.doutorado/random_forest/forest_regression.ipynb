{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9e04931",
   "metadata": {},
   "source": [
    "## Lab 5\n",
    "In this lab, you will implement the random forest algorithm to build models for regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa6737ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sbs\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ed8a13",
   "metadata": {},
   "source": [
    "Import the data in 'ENB2012_data.csv', split it into training, testing and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4cf218",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded. X shape: (768, 8), Y1 shape: (768,), Y2 shape: (768,)\n",
      "Training set size: 614 samples\n",
      "Validation set size: 77 samples\n",
      "Testing set size: 77 samples\n"
     ]
    }
   ],
   "source": [
    "#your code here\n",
    "data = pd.read_csv('ENB2012_data.csv')\n",
    "\n",
    "X = data[['X1', 'X2', 'X3', 'X4', 'X5', 'X6', 'X7', 'X8']].values\n",
    "y1 = data['Y1'].values  # Heating Load\n",
    "y2 = data['Y2'].values  # Cooling Load\n",
    "\n",
    "y_combined = np.column_stack((y1, y2))\n",
    "\n",
    "# Split data into training, testing, and validation sets\n",
    "X_train_full, X_temp, y_combined_train_full, y_combined_temp = train_test_split(X, y_combined, train_size=0.8, random_state=42)\n",
    "\n",
    "X_val, X_test, y_combined_val, y_combined_test = train_test_split(X_temp, y_combined_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Unpack\n",
    "y1_train_full, y2_train_full = y_combined_train_full[:, 0], y_combined_train_full[:, 1]\n",
    "y1_val, y2_val = y_combined_val[:, 0], y_combined_val[:, 1]\n",
    "y1_test, y2_test = y_combined_test[:, 0], y_combined_test[:, 1]\n",
    "\n",
    "print(f\"Dataset loaded. X shape: {X.shape}, Y1 shape: {y1.shape}, Y2 shape: {y2.shape}\")\n",
    "print(f\"Training set size: {X_train_full.shape[0]} samples\")\n",
    "print(f\"Validation set size: {X_val.shape[0]} samples\")\n",
    "print(f\"Testing set size: {X_test.shape[0]} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eefa4cb2",
   "metadata": {},
   "source": [
    "Built the function you need to train a normal decision tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29b9137",
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code goes here\n",
    "def bootstrap(X, num_bags=10, random_seed=0):\n",
    "    \"\"\"\n",
    "    Given a dataset and a number of bags, sample the dataset with replacement.\n",
    "    This function returns a list of indices with compatible dimensionality.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : ndarray\n",
    "        A dataset\n",
    "    num_bags : int, default 10\n",
    "        The number of bags to create\n",
    "    random_seed : int, default 0\n",
    "        Seed for the random number generator to ensure reproducibility.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list of ndarray\n",
    "        The list contains `num_bags` integer one-dimensional ndarrays.\n",
    "        Each of these contains the indices corresponding to the sampled datapoints in `X`.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    * The number of datapoints in each bag will match the number of datapoints in the given dataset.\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(random_seed)\n",
    "    num_samples = len(X)\n",
    "    bags = []\n",
    "    for _ in range(num_bags):\n",
    "        # Generate indices with replacement\n",
    "        indices = rng.choice(num_samples, size=num_samples, replace=True)\n",
    "        bags.append(indices)\n",
    "    return bags\n",
    "\n",
    "\n",
    "def aggregate_regression(preds):\n",
    "    \"\"\"\n",
    "    Aggregate predictions by several estimators.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    preds : list of ndarray\n",
    "        Predictions from multiple estimators. All ndarrays in this list should have the same\n",
    "        dimensionality.\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    ndarray\n",
    "        The mean of the predictions.\n",
    "    \"\"\"\n",
    "    # Calculate the mean across all predictions\n",
    "    return np.mean(preds, axis=0)\n",
    "\n",
    "def regression_criterion(region):\n",
    "    \"\"\"\n",
    "    Implements the sum of squared error criterion in a region.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    region : ndarray\n",
    "        Array of shape (N,) containing the values of the target values\n",
    "        for N datapoints in the training set.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        The sum of squared error.\n",
    "\n",
    "    Note\n",
    "    ----\n",
    "    The error for an empty region should be infinity (use: float(\"inf\")).\n",
    "    This avoids creating empty regions.\n",
    "    \"\"\"\n",
    "    if len(region) == 0:\n",
    "        return float(\"inf\")\n",
    "\n",
    "    if isinstance(region, pd.DataFrame) or isinstance(region, pd.Series):\n",
    "        region = region.values\n",
    "\n",
    "    mean_value = np.mean(region)\n",
    "    squared_errors = (region - mean_value) ** 2\n",
    "\n",
    "    return np.sum(squared_errors)\n",
    "\n",
    "\n",
    "def split_region(region_data, feature_index, tau):\n",
    "    \"\"\"\n",
    "    Given a region, splits it based on the feature indicated by\n",
    "    `feature_index`, the region will be split in two, where\n",
    "    one side will contain all points with the feature with values\n",
    "    lower than `tau`, and the other split will contain the\n",
    "    remaining datapoints.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    region_data : array of size (n_samples, n_features + 1)\n",
    "        a partition of the dataset (or the full dataset) to be split,\n",
    "        including the target variable as the last column.\n",
    "    feature_index : int\n",
    "        the index of the feature (column of the region array) used to make this partition\n",
    "    tau : float\n",
    "        The threshold used to make this partition\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    left_partition : array\n",
    "        datapoints in `region_data` where feature <= `tau`\n",
    "    right_partition : array\n",
    "        datapoints in `region_data` where feature > `tau`\n",
    "    \"\"\"\n",
    "    if isinstance(region_data, pd.DataFrame):\n",
    "        feature_values = region_data.iloc[:, feature_index].values\n",
    "    else:\n",
    "        feature_values = region_data[:, feature_index]\n",
    "\n",
    "    left_indices = np.where(feature_values <= tau)[0]\n",
    "    right_indices = np.where(feature_values > tau)[0]\n",
    "\n",
    "    left_partition = region_data[left_indices]\n",
    "    right_partition = region_data[right_indices]\n",
    "\n",
    "    return left_partition, right_partition\n",
    "\n",
    "\n",
    "class TreeNode:\n",
    "    \"\"\"\n",
    "    Represents a node in the Decision Tree.\n",
    "    \"\"\"\n",
    "    def __init__(self, data, feature_idx, feature_val, sq_error, is_leaf=False):\n",
    "        self.data = data\n",
    "        self.feature_idx = feature_idx\n",
    "        self.feature_val = feature_val\n",
    "        self.sq_error = sq_error\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "        self._is_leaf = is_leaf\n",
    "\n",
    "    @property\n",
    "    def is_leaf(self):\n",
    "        \"\"\"\n",
    "        Determines if the node is a leaf node.\n",
    "        A node is a leaf if it was explicitly marked as such, or if it has no children.\n",
    "        \"\"\"\n",
    "        return self._is_leaf or (self.left is None and self.right is None)\n",
    "\n",
    "\n",
    "class DecisionTree:\n",
    "    \"\"\"\n",
    "    Custom Decision Tree Regressor implementation.\n",
    "    \"\"\"\n",
    "    def __init__(self, criterion=regression_criterion, max_depth=None, min_samples_leaf=1, num_features=None, random_state=None):\n",
    "        \"\"\"\n",
    "        Initializes the DecisionTree Regressor.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        criterion : function, default regression_criterion\n",
    "            The function used to evaluate the quality of a split (e.g., sum of squared errors).\n",
    "        max_depth : int, default None\n",
    "            The maximum depth of the tree. If None, then nodes are expanded until\n",
    "            all leaves are pure or until all leaves contain less than min_samples_leaf samples.\n",
    "        min_samples_leaf : int, default 1\n",
    "            The minimum number of samples required to be at a leaf node.\n",
    "        num_features : int or float or str, default None\n",
    "            The number of features to consider when looking for the best split:\n",
    "            - If int, then consider `num_features` features at each split.\n",
    "            - If float, then `num_features` is a fraction and `int(num_features * n_features)` features are considered.\n",
    "            - If \"auto\", then `num_features=sqrt(n_features)`.\n",
    "            - If \"sqrt\", then `num_features=sqrt(n_features)`.\n",
    "            - If \"log2\", then `num_features=log2(n_features)`.\n",
    "            - If None, then all features are considered.\n",
    "        random_state : int, default None\n",
    "            Controls the random seed for reproducibility of feature selection within the tree.\n",
    "        \"\"\"\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.criterion = criterion\n",
    "        self.tree = None\n",
    "        self.num_features = num_features\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        \"\"\"\n",
    "        Builds the decision tree from the training data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X_train : ndarray\n",
    "            Training features.\n",
    "        y_train : ndarray\n",
    "            Training target values.\n",
    "        \"\"\"\n",
    "        train_data = np.column_stack((X_train, y_train))\n",
    "        self.tree = self.create_tree(train_data, current_depth=0)\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        \"\"\"\n",
    "        Predicts target values for new data using the trained Decision Tree.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X_test : ndarray\n",
    "            Features of the new data points.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        ndarray\n",
    "            Predicted target values.\n",
    "        \"\"\"\n",
    "        if self.tree is None:\n",
    "            raise RuntimeError(\"Decision Tree not fitted. Call fit() before predict().\")\n",
    "\n",
    "        predictions = np.array([self.predict_sample(x, self.tree) for x in X_test])\n",
    "        return predictions\n",
    "\n",
    "    def create_tree(self, data, current_depth):\n",
    "        \"\"\"\n",
    "        Recursively creates the decision tree.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        data : ndarray\n",
    "            The subset of data for the current node (features and target).\n",
    "        current_depth : int\n",
    "            The current depth of the node in the tree.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        TreeNode\n",
    "            The created node (either internal or leaf).\n",
    "        \"\"\"\n",
    "        # Calculate sum of squared error for the current node's data\n",
    "        current_node_sq_error = self.criterion(data[:, -1])\n",
    "\n",
    "        # Stopping criteria:\n",
    "        # 1. Max depth reached\n",
    "        if self.max_depth is not None and current_depth >= self.max_depth:\n",
    "            return TreeNode(data, None, None, current_node_sq_error, is_leaf=True)\n",
    "\n",
    "        # 2. Minimum samples per leaf reached\n",
    "        if len(data) <= self.min_samples_leaf:\n",
    "            return TreeNode(data, None, None, current_node_sq_error, is_leaf=True)\n",
    "\n",
    "        # Find the best split for the current data\n",
    "        split_data_tuple, split_feature_idx, split_feature_val, best_split_sq_error = self.find_bestsplit(data)\n",
    "\n",
    "        # 3. No valid split found or no improvement\n",
    "        if split_feature_idx is None:\n",
    "            return TreeNode(data, None, None, current_node_sq_error, is_leaf=True)\n",
    "\n",
    "        # Create the current node\n",
    "        node = TreeNode(data, split_feature_idx, split_feature_val, current_node_sq_error)\n",
    "\n",
    "        # 4. Check if splits result in regions smaller than min_samples_leaf\n",
    "        # This check ensures that even if a split is found, if it leads to\n",
    "        # children nodes that are too small, we make the current node a leaf.\n",
    "        left_data, right_data = split_data_tuple\n",
    "        if len(left_data) < self.min_samples_leaf or len(right_data) < self.min_samples_leaf:\n",
    "             return TreeNode(data, None, None, current_node_sq_error, is_leaf=True)\n",
    "\n",
    "        # Recursively build left and right children\n",
    "        node.left = self.create_tree(left_data, current_depth + 1)\n",
    "        node.right = self.create_tree(right_data, current_depth + 1)\n",
    "\n",
    "        return node\n",
    "\n",
    "    def find_bestsplit(self, data):\n",
    "        \"\"\"\n",
    "        Finds the best feature and threshold to split the given data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        data : ndarray\n",
    "            The subset of data for which to find the best split.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        tuple\n",
    "            (regions, best_feature_index, best_tau, current_node_sq_error)\n",
    "            regions : tuple of ndarrays (left_partition, right_partition)\n",
    "            best_feature_index : int or None\n",
    "            best_tau : float or None\n",
    "            current_node_sq_error : float (sum of squared error for the current node's data)\n",
    "        \"\"\"\n",
    "        if isinstance(data, pd.DataFrame):\n",
    "            data = data.values\n",
    "        if isinstance(data, pd.Series):\n",
    "            data = data.values\n",
    "\n",
    "        best_sq_error = float('inf')\n",
    "        best_feature_index = None\n",
    "        best_tau = None\n",
    "        regions = None\n",
    "\n",
    "        n_samples, n_total_cols = data.shape\n",
    "        n_features = n_total_cols - 1  # Exclude target column (last column)\n",
    "\n",
    "        # Calculate sum of squared error for the current node's data before splitting\n",
    "        current_node_sq_error = self.criterion(data[:, -1])\n",
    "\n",
    "        # Determine the actual number of features to sample for this split\n",
    "        num_features_for_split = n_features  # Default to all features if num_features is None or invalid\n",
    "        if self.num_features is not None:\n",
    "            if isinstance(self.num_features, int):\n",
    "                num_features_for_split = min(self.num_features, n_features)\n",
    "            elif isinstance(self.num_features, float):\n",
    "                num_features_for_split = int(self.num_features * n_features)\n",
    "            elif self.num_features == \"auto\" or self.num_features == \"sqrt\":\n",
    "                num_features_for_split = int(np.sqrt(n_features))\n",
    "            elif self.num_features == \"log2\":\n",
    "                num_features_for_split = int(np.log2(n_features))\n",
    "\n",
    "        # Handle case where num_features_for_split might be 0 or negative\n",
    "        if num_features_for_split <= 0:\n",
    "            return None, None, None, current_node_sq_error # No valid split possible\n",
    "\n",
    "        # Randomly select a subset of features for this split\n",
    "        rng_split = np.random.default_rng(self.random_state)\n",
    "        all_feature_indices = np.arange(n_features)\n",
    "        selected_feature_indices = rng_split.choice(all_feature_indices, size=num_features_for_split, replace=False)\n",
    "\n",
    "        for feature_index in selected_feature_indices:  # Iterate only over selected features\n",
    "            feature_values = data[:, feature_index]\n",
    "            possible_taus = np.unique(feature_values)\n",
    "\n",
    "            # Avoid splitting on features with only one unique value (no meaningful split)\n",
    "            if len(possible_taus) < 2:\n",
    "                continue\n",
    "\n",
    "            for tau in possible_taus:\n",
    "                left, right = split_region(data, feature_index, tau) # Use the global split_region function\n",
    "\n",
    "                # Ensure splits are not empty before evaluating criterion\n",
    "                if len(left) == 0 or len(right) == 0:\n",
    "                    continue  # Skip if split creates empty regions\n",
    "\n",
    "                sq_error = self.criterion(left[:, -1]) + self.criterion(right[:, -1])\n",
    "\n",
    "                if sq_error < best_sq_error:\n",
    "                    best_sq_error = sq_error\n",
    "                    best_feature_index = feature_index\n",
    "                    best_tau = tau\n",
    "                    regions = (left, right)\n",
    "\n",
    "        # If no split was found that improves the criterion (best_sq_error remains inf)\n",
    "        # or if the best split found is not better than the current node's error (no gain)\n",
    "        if best_feature_index is None or best_sq_error >= current_node_sq_error:\n",
    "            return None, None, None, current_node_sq_error  # Indicate no valid split found\n",
    "\n",
    "        return regions, best_feature_index, best_tau, current_node_sq_error\n",
    "\n",
    "    def predict_sample(self, sample, node):\n",
    "        \"\"\"\n",
    "        Recursively predicts the target value for a single sample.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        sample : ndarray\n",
    "            A single data point (features only).\n",
    "        node : TreeNode\n",
    "            The current node in the decision tree.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "            The predicted target value for the sample.\n",
    "        \"\"\"\n",
    "        if node.is_leaf:\n",
    "            # For a leaf node, the prediction is the mean of the target values in that node's data\n",
    "            return np.mean(node.data[:, -1])\n",
    "\n",
    "        # Traverse the tree based on the feature and threshold\n",
    "        if sample[node.feature_idx] <= node.feature_val: # Changed to <= as per split_region\n",
    "            if node.left is None: # Handle cases where a branch might be None (e.g., due to max_depth or min_samples_leaf)\n",
    "                return np.mean(node.data[:, -1]) # Return current node's mean if child is None\n",
    "            return self.predict_sample(sample, node.left)\n",
    "        else:\n",
    "            if node.right is None: # Handle cases where a branch might be None\n",
    "                return np.mean(node.data[:, -1])\n",
    "            return self.predict_sample(sample, node.right)\n",
    "\n",
    "    def print_tree(self):\n",
    "        \"\"\"Prints the structure of the trained decision tree.\"\"\"\n",
    "        self._print_tree(self.tree)\n",
    "\n",
    "    def _print_tree(self, node, depth=0, prefix=\"\"):\n",
    "        \"\"\"Helper function for recursively printing the tree.\"\"\"\n",
    "        if node is None:\n",
    "            return\n",
    "        indent = \"  \" * depth\n",
    "        if node.is_leaf:\n",
    "            print(f\"{indent}{prefix}Leaf: Predict {np.mean(node.data[:, -1]):.2f}, Samples: {len(node.data)}, SSE = {node.sq_error:.2f}\")\n",
    "        else:\n",
    "            print(f\"{indent}{prefix}Node: Feature {node.feature_idx}, Threshold {node.feature_val:.2f}, SSE = {node.sq_error:.2f}\")\n",
    "            self._print_tree(node.left, depth + 1, prefix=\"L--> \")\n",
    "            self._print_tree(node.right, depth + 1, prefix=\"R--> \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3dcd499",
   "metadata": {},
   "source": [
    "Buit a function that train a random forest and predicts the electric loads of a new point given a trained random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16520f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code goes here\n",
    "class RandomForest:\n",
    "    \"\"\"\n",
    "    Custom Random Forest Regressor implementation.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_features=None, min_samples_leaf=1, max_depth=None, num_estimators=10, random_state=None):\n",
    "        \"\"\"\n",
    "        Initializes the RandomForest Regressor.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        num_features : int or float or str, default None\n",
    "            The number of features to consider when looking for the best split in each tree.\n",
    "            Passed to the underlying DecisionTreeRegressor.\n",
    "        min_samples_leaf : int, default 1\n",
    "            The minimum number of samples required to be at a leaf node for each tree.\n",
    "        max_depth : int, default None\n",
    "            The maximum depth of each decision tree in the forest.\n",
    "        num_estimators : int, default 10\n",
    "            The number of decision trees (estimators) in the forest.\n",
    "        random_state : int, default None\n",
    "            Controls the random seed for reproducibility of the overall Random Forest.\n",
    "            Used for bootstrapping and for passing unique seeds to individual trees.\n",
    "        \"\"\"\n",
    "        self.num_features = num_features\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.max_depth = max_depth\n",
    "        self.num_estimators = num_estimators\n",
    "        self.random_state = random_state\n",
    "        self.estimators = []  # List to store trained DecisionTree models\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        \"\"\"\n",
    "        Trains the Random Forest model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X_train : ndarray\n",
    "            Training features.\n",
    "        y_train : ndarray\n",
    "            Training target values.\n",
    "        \"\"\"\n",
    "        self.estimators = []  # Clear any existing estimators\n",
    "        rng = np.random.default_rng(self.random_state)  # Random number generator for reproducibility\n",
    "\n",
    "        # Generate all bags of indices upfront\n",
    "        all_bag_indices = bootstrap(X_train, num_bags=self.num_estimators, random_seed=self.random_state)\n",
    "\n",
    "        for i in range(self.num_estimators):\n",
    "            # Get the data for the current bag\n",
    "            bag_indices = all_bag_indices[i]\n",
    "            bag_X = X_train[bag_indices]\n",
    "            bag_y = y_train[bag_indices]\n",
    "\n",
    "            # Build and train a custom DecisionTree for the current bag\n",
    "            # Pass a unique random state to each tree for its internal feature selection\n",
    "            tree_random_state = rng.integers(0, 1000000) # Use a large range for seeds\n",
    "            rf_estimator = DecisionTree(\n",
    "                max_depth=self.max_depth,\n",
    "                min_samples_leaf=self.min_samples_leaf,\n",
    "                num_features=self.num_features,\n",
    "                random_state=tree_random_state\n",
    "            )\n",
    "            rf_estimator.fit(bag_X, bag_y)\n",
    "            self.estimators.append(rf_estimator)\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        \"\"\"\n",
    "        Predicts target values for new data using the trained Random Forest.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X_test : ndarray\n",
    "            Features of the new data points.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        ndarray\n",
    "            Predicted target values.\n",
    "        \"\"\"\n",
    "        if not self.estimators:\n",
    "            raise RuntimeError(\"Random Forest not fitted. Call fit() before predict().\")\n",
    "\n",
    "        predictions = []\n",
    "        for estimator in self.estimators:\n",
    "            # Get predictions from each individual tree\n",
    "            pred = estimator.predict(X_test)\n",
    "            predictions.append(pred)\n",
    "        # Aggregate predictions using the mean\n",
    "        return aggregate_regression(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec10172c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculates the Root Mean Squared Error (RMSE).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : ndarray\n",
    "        Actual target values.\n",
    "    y_pred : ndarray\n",
    "        Predicted target values.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        The RMSE value.\n",
    "    \"\"\"\n",
    "    # Ensure y_true and y_pred are numpy arrays for consistent operations\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_pred = np.asarray(y_pred)\n",
    "    return np.sqrt(np.mean((y_true - y_pred) ** 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd6240a",
   "metadata": {},
   "source": [
    "Builld functions to evaluate your predictions, use this functions to find optimal hyperparameters using also our validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de8d5586",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Y1: n_est=10, max_depth=5, min_leaf=1, n_feat=0.5\n",
      "  Validation RMSE Y1: 1.4125\n",
      "Testing Y1: n_est=10, max_depth=5, min_leaf=1, n_feat=0.7\n",
      "  Validation RMSE Y1: 1.2922\n",
      "Testing Y1: n_est=10, max_depth=5, min_leaf=1, n_feat=1.0\n",
      "  Validation RMSE Y1: 1.0542\n",
      "Testing Y1: n_est=10, max_depth=5, min_leaf=5, n_feat=0.5\n",
      "  Validation RMSE Y1: 1.5310\n",
      "Testing Y1: n_est=10, max_depth=5, min_leaf=5, n_feat=0.7\n",
      "  Validation RMSE Y1: 1.4216\n",
      "Testing Y1: n_est=10, max_depth=5, min_leaf=5, n_feat=1.0\n",
      "  Validation RMSE Y1: 1.1948\n",
      "Testing Y1: n_est=10, max_depth=5, min_leaf=10, n_feat=0.5\n",
      "  Validation RMSE Y1: 1.9456\n",
      "Testing Y1: n_est=10, max_depth=5, min_leaf=10, n_feat=0.7\n",
      "  Validation RMSE Y1: 1.7748\n",
      "Testing Y1: n_est=10, max_depth=5, min_leaf=10, n_feat=1.0\n",
      "  Validation RMSE Y1: 1.6447\n",
      "Testing Y1: n_est=10, max_depth=10, min_leaf=1, n_feat=0.5\n",
      "  Validation RMSE Y1: 1.1828\n",
      "Testing Y1: n_est=10, max_depth=10, min_leaf=1, n_feat=0.7\n",
      "  Validation RMSE Y1: 1.0210\n",
      "Testing Y1: n_est=10, max_depth=10, min_leaf=1, n_feat=1.0\n",
      "  Validation RMSE Y1: 0.5691\n",
      "Testing Y1: n_est=10, max_depth=10, min_leaf=5, n_feat=0.5\n",
      "  Validation RMSE Y1: 1.3462\n",
      "Testing Y1: n_est=10, max_depth=10, min_leaf=5, n_feat=0.7\n",
      "  Validation RMSE Y1: 1.1486\n",
      "Testing Y1: n_est=10, max_depth=10, min_leaf=5, n_feat=1.0\n",
      "  Validation RMSE Y1: 0.7767\n",
      "Testing Y1: n_est=10, max_depth=10, min_leaf=10, n_feat=0.5\n",
      "  Validation RMSE Y1: 1.8020\n",
      "Testing Y1: n_est=10, max_depth=10, min_leaf=10, n_feat=0.7\n",
      "  Validation RMSE Y1: 1.5989\n",
      "Testing Y1: n_est=10, max_depth=10, min_leaf=10, n_feat=1.0\n",
      "  Validation RMSE Y1: 1.3886\n",
      "Testing Y1: n_est=10, max_depth=15, min_leaf=1, n_feat=0.5\n",
      "  Validation RMSE Y1: 1.1820\n",
      "Testing Y1: n_est=10, max_depth=15, min_leaf=1, n_feat=0.7\n",
      "  Validation RMSE Y1: 1.0251\n",
      "Testing Y1: n_est=10, max_depth=15, min_leaf=1, n_feat=1.0\n",
      "  Validation RMSE Y1: 0.5668\n",
      "Testing Y1: n_est=10, max_depth=15, min_leaf=5, n_feat=0.5\n",
      "  Validation RMSE Y1: 1.3462\n",
      "Testing Y1: n_est=10, max_depth=15, min_leaf=5, n_feat=0.7\n",
      "  Validation RMSE Y1: 1.1486\n",
      "Testing Y1: n_est=10, max_depth=15, min_leaf=5, n_feat=1.0\n",
      "  Validation RMSE Y1: 0.7767\n",
      "Testing Y1: n_est=10, max_depth=15, min_leaf=10, n_feat=0.5\n",
      "  Validation RMSE Y1: 1.8020\n",
      "Testing Y1: n_est=10, max_depth=15, min_leaf=10, n_feat=0.7\n",
      "  Validation RMSE Y1: 1.5989\n",
      "Testing Y1: n_est=10, max_depth=15, min_leaf=10, n_feat=1.0\n",
      "  Validation RMSE Y1: 1.3886\n",
      "Testing Y1: n_est=10, max_depth=None, min_leaf=1, n_feat=0.5\n",
      "  Validation RMSE Y1: 1.1820\n",
      "Testing Y1: n_est=10, max_depth=None, min_leaf=1, n_feat=0.7\n",
      "  Validation RMSE Y1: 1.0251\n",
      "Testing Y1: n_est=10, max_depth=None, min_leaf=1, n_feat=1.0\n",
      "  Validation RMSE Y1: 0.5668\n",
      "Testing Y1: n_est=10, max_depth=None, min_leaf=5, n_feat=0.5\n",
      "  Validation RMSE Y1: 1.3462\n",
      "Testing Y1: n_est=10, max_depth=None, min_leaf=5, n_feat=0.7\n",
      "  Validation RMSE Y1: 1.1486\n",
      "Testing Y1: n_est=10, max_depth=None, min_leaf=5, n_feat=1.0\n",
      "  Validation RMSE Y1: 0.7767\n",
      "Testing Y1: n_est=10, max_depth=None, min_leaf=10, n_feat=0.5\n",
      "  Validation RMSE Y1: 1.8020\n",
      "Testing Y1: n_est=10, max_depth=None, min_leaf=10, n_feat=0.7\n",
      "  Validation RMSE Y1: 1.5989\n",
      "Testing Y1: n_est=10, max_depth=None, min_leaf=10, n_feat=1.0\n",
      "  Validation RMSE Y1: 1.3886\n",
      "Testing Y1: n_est=20, max_depth=5, min_leaf=1, n_feat=0.5\n",
      "  Validation RMSE Y1: 1.6080\n",
      "Testing Y1: n_est=20, max_depth=5, min_leaf=1, n_feat=0.7\n",
      "  Validation RMSE Y1: 1.5817\n",
      "Testing Y1: n_est=20, max_depth=5, min_leaf=1, n_feat=1.0\n",
      "  Validation RMSE Y1: 1.0667\n",
      "Testing Y1: n_est=20, max_depth=5, min_leaf=5, n_feat=0.5\n",
      "  Validation RMSE Y1: 1.7228\n",
      "Testing Y1: n_est=20, max_depth=5, min_leaf=5, n_feat=0.7\n",
      "  Validation RMSE Y1: 1.6990\n",
      "Testing Y1: n_est=20, max_depth=5, min_leaf=5, n_feat=1.0\n",
      "  Validation RMSE Y1: 1.1943\n",
      "Testing Y1: n_est=20, max_depth=5, min_leaf=10, n_feat=0.5\n",
      "  Validation RMSE Y1: 2.0605\n",
      "Testing Y1: n_est=20, max_depth=5, min_leaf=10, n_feat=0.7\n",
      "  Validation RMSE Y1: 2.0049\n",
      "Testing Y1: n_est=20, max_depth=5, min_leaf=10, n_feat=1.0\n",
      "  Validation RMSE Y1: 1.6881\n",
      "Testing Y1: n_est=20, max_depth=10, min_leaf=1, n_feat=0.5\n",
      "  Validation RMSE Y1: 1.4538\n",
      "Testing Y1: n_est=20, max_depth=10, min_leaf=1, n_feat=0.7\n",
      "  Validation RMSE Y1: 1.5669\n",
      "Testing Y1: n_est=20, max_depth=10, min_leaf=1, n_feat=1.0\n",
      "  Validation RMSE Y1: 0.5370\n",
      "Testing Y1: n_est=20, max_depth=10, min_leaf=5, n_feat=0.5\n",
      "  Validation RMSE Y1: 1.6090\n",
      "Testing Y1: n_est=20, max_depth=10, min_leaf=5, n_feat=0.7\n",
      "  Validation RMSE Y1: 1.6067\n",
      "Testing Y1: n_est=20, max_depth=10, min_leaf=5, n_feat=1.0\n",
      "  Validation RMSE Y1: 0.7412\n",
      "Testing Y1: n_est=20, max_depth=10, min_leaf=10, n_feat=0.5\n",
      "  Validation RMSE Y1: 1.9688\n",
      "Testing Y1: n_est=20, max_depth=10, min_leaf=10, n_feat=0.7\n",
      "  Validation RMSE Y1: 1.9252\n",
      "Testing Y1: n_est=20, max_depth=10, min_leaf=10, n_feat=1.0\n",
      "  Validation RMSE Y1: 1.4180\n",
      "Testing Y1: n_est=20, max_depth=15, min_leaf=1, n_feat=0.5\n",
      "  Validation RMSE Y1: 1.4519\n",
      "Testing Y1: n_est=20, max_depth=15, min_leaf=1, n_feat=0.7\n",
      "  Validation RMSE Y1: 1.5766\n",
      "Testing Y1: n_est=20, max_depth=15, min_leaf=1, n_feat=1.0\n",
      "  Validation RMSE Y1: 0.5340\n",
      "Testing Y1: n_est=20, max_depth=15, min_leaf=5, n_feat=0.5\n",
      "  Validation RMSE Y1: 1.6090\n",
      "Testing Y1: n_est=20, max_depth=15, min_leaf=5, n_feat=0.7\n",
      "  Validation RMSE Y1: 1.6066\n",
      "Testing Y1: n_est=20, max_depth=15, min_leaf=5, n_feat=1.0\n",
      "  Validation RMSE Y1: 0.7412\n",
      "Testing Y1: n_est=20, max_depth=15, min_leaf=10, n_feat=0.5\n",
      "  Validation RMSE Y1: 1.9688\n",
      "Testing Y1: n_est=20, max_depth=15, min_leaf=10, n_feat=0.7\n",
      "  Validation RMSE Y1: 1.9252\n",
      "Testing Y1: n_est=20, max_depth=15, min_leaf=10, n_feat=1.0\n",
      "  Validation RMSE Y1: 1.4180\n",
      "Testing Y1: n_est=20, max_depth=None, min_leaf=1, n_feat=0.5\n",
      "  Validation RMSE Y1: 1.4519\n",
      "Testing Y1: n_est=20, max_depth=None, min_leaf=1, n_feat=0.7\n",
      "  Validation RMSE Y1: 1.5766\n",
      "Testing Y1: n_est=20, max_depth=None, min_leaf=1, n_feat=1.0\n",
      "  Validation RMSE Y1: 0.5340\n",
      "Testing Y1: n_est=20, max_depth=None, min_leaf=5, n_feat=0.5\n",
      "  Validation RMSE Y1: 1.6090\n",
      "Testing Y1: n_est=20, max_depth=None, min_leaf=5, n_feat=0.7\n",
      "  Validation RMSE Y1: 1.6066\n",
      "Testing Y1: n_est=20, max_depth=None, min_leaf=5, n_feat=1.0\n",
      "  Validation RMSE Y1: 0.7412\n",
      "Testing Y1: n_est=20, max_depth=None, min_leaf=10, n_feat=0.5\n",
      "  Validation RMSE Y1: 1.9688\n",
      "Testing Y1: n_est=20, max_depth=None, min_leaf=10, n_feat=0.7\n",
      "  Validation RMSE Y1: 1.9252\n",
      "Testing Y1: n_est=20, max_depth=None, min_leaf=10, n_feat=1.0\n",
      "  Validation RMSE Y1: 1.4180\n",
      "Testing Y1: n_est=50, max_depth=5, min_leaf=1, n_feat=0.5\n",
      "  Validation RMSE Y1: 1.6675\n",
      "Testing Y1: n_est=50, max_depth=5, min_leaf=1, n_feat=0.7\n",
      "  Validation RMSE Y1: 1.4214\n",
      "Testing Y1: n_est=50, max_depth=5, min_leaf=1, n_feat=1.0\n",
      "  Validation RMSE Y1: 1.0764\n",
      "Testing Y1: n_est=50, max_depth=5, min_leaf=5, n_feat=0.5\n",
      "  Validation RMSE Y1: 1.7820\n",
      "Testing Y1: n_est=50, max_depth=5, min_leaf=5, n_feat=0.7\n",
      "  Validation RMSE Y1: 1.5580\n",
      "Testing Y1: n_est=50, max_depth=5, min_leaf=5, n_feat=1.0\n",
      "  Validation RMSE Y1: 1.2034\n",
      "Testing Y1: n_est=50, max_depth=5, min_leaf=10, n_feat=0.5\n",
      "  Validation RMSE Y1: 2.1002\n",
      "Testing Y1: n_est=50, max_depth=5, min_leaf=10, n_feat=0.7\n",
      "  Validation RMSE Y1: 1.8635\n",
      "Testing Y1: n_est=50, max_depth=5, min_leaf=10, n_feat=1.0\n",
      "  Validation RMSE Y1: 1.6504\n",
      "Testing Y1: n_est=50, max_depth=10, min_leaf=1, n_feat=0.5\n",
      "  Validation RMSE Y1: 1.6198\n",
      "Testing Y1: n_est=50, max_depth=10, min_leaf=1, n_feat=0.7\n",
      "  Validation RMSE Y1: 1.2928\n",
      "Testing Y1: n_est=50, max_depth=10, min_leaf=1, n_feat=1.0\n",
      "  Validation RMSE Y1: 0.5490\n",
      "Testing Y1: n_est=50, max_depth=10, min_leaf=5, n_feat=0.5\n",
      "  Validation RMSE Y1: 1.7196\n",
      "Testing Y1: n_est=50, max_depth=10, min_leaf=5, n_feat=0.7\n",
      "  Validation RMSE Y1: 1.3883\n",
      "Testing Y1: n_est=50, max_depth=10, min_leaf=5, n_feat=1.0\n",
      "  Validation RMSE Y1: 0.7478\n",
      "Testing Y1: n_est=50, max_depth=10, min_leaf=10, n_feat=0.5\n",
      "  Validation RMSE Y1: 2.0312\n",
      "Testing Y1: n_est=50, max_depth=10, min_leaf=10, n_feat=0.7\n",
      "  Validation RMSE Y1: 1.7238\n",
      "Testing Y1: n_est=50, max_depth=10, min_leaf=10, n_feat=1.0\n",
      "  Validation RMSE Y1: 1.3690\n",
      "Testing Y1: n_est=50, max_depth=15, min_leaf=1, n_feat=0.5\n",
      "  Validation RMSE Y1: 1.6248\n",
      "Testing Y1: n_est=50, max_depth=15, min_leaf=1, n_feat=0.7\n",
      "  Validation RMSE Y1: 1.3044\n",
      "Testing Y1: n_est=50, max_depth=15, min_leaf=1, n_feat=1.0\n",
      "  Validation RMSE Y1: 0.5470\n",
      "Testing Y1: n_est=50, max_depth=15, min_leaf=5, n_feat=0.5\n",
      "  Validation RMSE Y1: 1.7196\n",
      "Testing Y1: n_est=50, max_depth=15, min_leaf=5, n_feat=0.7\n",
      "  Validation RMSE Y1: 1.3882\n",
      "Testing Y1: n_est=50, max_depth=15, min_leaf=5, n_feat=1.0\n",
      "  Validation RMSE Y1: 0.7478\n",
      "Testing Y1: n_est=50, max_depth=15, min_leaf=10, n_feat=0.5\n",
      "  Validation RMSE Y1: 2.0312\n",
      "Testing Y1: n_est=50, max_depth=15, min_leaf=10, n_feat=0.7\n",
      "  Validation RMSE Y1: 1.7238\n",
      "Testing Y1: n_est=50, max_depth=15, min_leaf=10, n_feat=1.0\n",
      "  Validation RMSE Y1: 1.3690\n",
      "Testing Y1: n_est=50, max_depth=None, min_leaf=1, n_feat=0.5\n",
      "  Validation RMSE Y1: 1.6248\n",
      "Testing Y1: n_est=50, max_depth=None, min_leaf=1, n_feat=0.7\n",
      "  Validation RMSE Y1: 1.3044\n",
      "Testing Y1: n_est=50, max_depth=None, min_leaf=1, n_feat=1.0\n",
      "  Validation RMSE Y1: 0.5470\n",
      "Testing Y1: n_est=50, max_depth=None, min_leaf=5, n_feat=0.5\n",
      "  Validation RMSE Y1: 1.7196\n",
      "Testing Y1: n_est=50, max_depth=None, min_leaf=5, n_feat=0.7\n",
      "  Validation RMSE Y1: 1.3882\n",
      "Testing Y1: n_est=50, max_depth=None, min_leaf=5, n_feat=1.0\n",
      "  Validation RMSE Y1: 0.7478\n",
      "Testing Y1: n_est=50, max_depth=None, min_leaf=10, n_feat=0.5\n",
      "  Validation RMSE Y1: 2.0312\n",
      "Testing Y1: n_est=50, max_depth=None, min_leaf=10, n_feat=0.7\n",
      "  Validation RMSE Y1: 1.7238\n",
      "Testing Y1: n_est=50, max_depth=None, min_leaf=10, n_feat=1.0\n",
      "  Validation RMSE Y1: 1.3690\n",
      "\n",
      "Optimal Hyperparameters for Y1: {'num_estimators': 20, 'max_depth': 15, 'min_samples_leaf': 1, 'num_features': 1.0}\n",
      "Best Validation RMSE for Y1: 0.5340\n",
      "\n",
      "--- Hyperparameter Tuning for Y2 (Cooling Load) ---\n",
      "Testing Y2: n_est=10, max_depth=5, min_leaf=1, n_feat=0.5\n",
      "  Validation RMSE Y2: 1.9624\n",
      "Testing Y2: n_est=10, max_depth=5, min_leaf=1, n_feat=0.7\n",
      "  Validation RMSE Y2: 1.9555\n",
      "Testing Y2: n_est=10, max_depth=5, min_leaf=1, n_feat=1.0\n",
      "  Validation RMSE Y2: 1.8759\n",
      "Testing Y2: n_est=10, max_depth=5, min_leaf=5, n_feat=0.5\n",
      "  Validation RMSE Y2: 2.0710\n",
      "Testing Y2: n_est=10, max_depth=5, min_leaf=5, n_feat=0.7\n",
      "  Validation RMSE Y2: 2.0344\n",
      "Testing Y2: n_est=10, max_depth=5, min_leaf=5, n_feat=1.0\n",
      "  Validation RMSE Y2: 1.9121\n",
      "Testing Y2: n_est=10, max_depth=5, min_leaf=10, n_feat=0.5\n",
      "  Validation RMSE Y2: 2.1227\n",
      "Testing Y2: n_est=10, max_depth=5, min_leaf=10, n_feat=0.7\n",
      "  Validation RMSE Y2: 2.0382\n",
      "Testing Y2: n_est=10, max_depth=5, min_leaf=10, n_feat=1.0\n",
      "  Validation RMSE Y2: 1.9102\n",
      "Testing Y2: n_est=10, max_depth=10, min_leaf=1, n_feat=0.5\n",
      "  Validation RMSE Y2: 2.0239\n",
      "Testing Y2: n_est=10, max_depth=10, min_leaf=1, n_feat=0.7\n",
      "  Validation RMSE Y2: 1.8080\n",
      "Testing Y2: n_est=10, max_depth=10, min_leaf=1, n_feat=1.0\n",
      "  Validation RMSE Y2: 1.8294\n",
      "Testing Y2: n_est=10, max_depth=10, min_leaf=5, n_feat=0.5\n",
      "  Validation RMSE Y2: 2.0973\n",
      "Testing Y2: n_est=10, max_depth=10, min_leaf=5, n_feat=0.7\n",
      "  Validation RMSE Y2: 2.0240\n",
      "Testing Y2: n_est=10, max_depth=10, min_leaf=5, n_feat=1.0\n",
      "  Validation RMSE Y2: 1.9694\n",
      "Testing Y2: n_est=10, max_depth=10, min_leaf=10, n_feat=0.5\n",
      "  Validation RMSE Y2: 2.1255\n",
      "Testing Y2: n_est=10, max_depth=10, min_leaf=10, n_feat=0.7\n",
      "  Validation RMSE Y2: 2.0208\n",
      "Testing Y2: n_est=10, max_depth=10, min_leaf=10, n_feat=1.0\n",
      "  Validation RMSE Y2: 1.8964\n",
      "Testing Y2: n_est=10, max_depth=15, min_leaf=1, n_feat=0.5\n",
      "  Validation RMSE Y2: 2.0248\n",
      "Testing Y2: n_est=10, max_depth=15, min_leaf=1, n_feat=0.7\n",
      "  Validation RMSE Y2: 1.7851\n",
      "Testing Y2: n_est=10, max_depth=15, min_leaf=1, n_feat=1.0\n",
      "  Validation RMSE Y2: 1.8188\n",
      "Testing Y2: n_est=10, max_depth=15, min_leaf=5, n_feat=0.5\n",
      "  Validation RMSE Y2: 2.0973\n",
      "Testing Y2: n_est=10, max_depth=15, min_leaf=5, n_feat=0.7\n",
      "  Validation RMSE Y2: 2.0240\n",
      "Testing Y2: n_est=10, max_depth=15, min_leaf=5, n_feat=1.0\n",
      "  Validation RMSE Y2: 1.9694\n",
      "Testing Y2: n_est=10, max_depth=15, min_leaf=10, n_feat=0.5\n",
      "  Validation RMSE Y2: 2.1255\n",
      "Testing Y2: n_est=10, max_depth=15, min_leaf=10, n_feat=0.7\n",
      "  Validation RMSE Y2: 2.0208\n",
      "Testing Y2: n_est=10, max_depth=15, min_leaf=10, n_feat=1.0\n",
      "  Validation RMSE Y2: 1.8964\n",
      "Testing Y2: n_est=10, max_depth=None, min_leaf=1, n_feat=0.5\n",
      "  Validation RMSE Y2: 2.0248\n",
      "Testing Y2: n_est=10, max_depth=None, min_leaf=1, n_feat=0.7\n",
      "  Validation RMSE Y2: 1.7851\n",
      "Testing Y2: n_est=10, max_depth=None, min_leaf=1, n_feat=1.0\n",
      "  Validation RMSE Y2: 1.8188\n",
      "Testing Y2: n_est=10, max_depth=None, min_leaf=5, n_feat=0.5\n",
      "  Validation RMSE Y2: 2.0973\n",
      "Testing Y2: n_est=10, max_depth=None, min_leaf=5, n_feat=0.7\n",
      "  Validation RMSE Y2: 2.0240\n",
      "Testing Y2: n_est=10, max_depth=None, min_leaf=5, n_feat=1.0\n",
      "  Validation RMSE Y2: 1.9694\n",
      "Testing Y2: n_est=10, max_depth=None, min_leaf=10, n_feat=0.5\n",
      "  Validation RMSE Y2: 2.1255\n",
      "Testing Y2: n_est=10, max_depth=None, min_leaf=10, n_feat=0.7\n",
      "  Validation RMSE Y2: 2.0208\n",
      "Testing Y2: n_est=10, max_depth=None, min_leaf=10, n_feat=1.0\n",
      "  Validation RMSE Y2: 1.8964\n",
      "Testing Y2: n_est=20, max_depth=5, min_leaf=1, n_feat=0.5\n",
      "  Validation RMSE Y2: 2.0606\n",
      "Testing Y2: n_est=20, max_depth=5, min_leaf=1, n_feat=0.7\n",
      "  Validation RMSE Y2: 2.0890\n",
      "Testing Y2: n_est=20, max_depth=5, min_leaf=1, n_feat=1.0\n",
      "  Validation RMSE Y2: 1.8509\n",
      "Testing Y2: n_est=20, max_depth=5, min_leaf=5, n_feat=0.5\n",
      "  Validation RMSE Y2: 2.1541\n",
      "Testing Y2: n_est=20, max_depth=5, min_leaf=5, n_feat=0.7\n",
      "  Validation RMSE Y2: 2.1715\n",
      "Testing Y2: n_est=20, max_depth=5, min_leaf=5, n_feat=1.0\n",
      "  Validation RMSE Y2: 1.8962\n",
      "Testing Y2: n_est=20, max_depth=5, min_leaf=10, n_feat=0.5\n",
      "  Validation RMSE Y2: 2.2102\n",
      "Testing Y2: n_est=20, max_depth=5, min_leaf=10, n_feat=0.7\n",
      "  Validation RMSE Y2: 2.2105\n",
      "Testing Y2: n_est=20, max_depth=5, min_leaf=10, n_feat=1.0\n",
      "  Validation RMSE Y2: 1.9139\n",
      "Testing Y2: n_est=20, max_depth=10, min_leaf=1, n_feat=0.5\n",
      "  Validation RMSE Y2: 2.0641\n",
      "Testing Y2: n_est=20, max_depth=10, min_leaf=1, n_feat=0.7\n",
      "  Validation RMSE Y2: 1.9680\n",
      "Testing Y2: n_est=20, max_depth=10, min_leaf=1, n_feat=1.0\n",
      "  Validation RMSE Y2: 1.8155\n",
      "Testing Y2: n_est=20, max_depth=10, min_leaf=5, n_feat=0.5\n",
      "  Validation RMSE Y2: 2.1535\n",
      "Testing Y2: n_est=20, max_depth=10, min_leaf=5, n_feat=0.7\n",
      "  Validation RMSE Y2: 2.1544\n",
      "Testing Y2: n_est=20, max_depth=10, min_leaf=5, n_feat=1.0\n",
      "  Validation RMSE Y2: 1.9844\n",
      "Testing Y2: n_est=20, max_depth=10, min_leaf=10, n_feat=0.5\n",
      "  Validation RMSE Y2: 2.2285\n",
      "Testing Y2: n_est=20, max_depth=10, min_leaf=10, n_feat=0.7\n",
      "  Validation RMSE Y2: 2.2098\n",
      "Testing Y2: n_est=20, max_depth=10, min_leaf=10, n_feat=1.0\n",
      "  Validation RMSE Y2: 1.9233\n",
      "Testing Y2: n_est=20, max_depth=15, min_leaf=1, n_feat=0.5\n",
      "  Validation RMSE Y2: 2.0647\n",
      "Testing Y2: n_est=20, max_depth=15, min_leaf=1, n_feat=0.7\n",
      "  Validation RMSE Y2: 1.9548\n",
      "Testing Y2: n_est=20, max_depth=15, min_leaf=1, n_feat=1.0\n",
      "  Validation RMSE Y2: 1.8049\n",
      "Testing Y2: n_est=20, max_depth=15, min_leaf=5, n_feat=0.5\n",
      "  Validation RMSE Y2: 2.1535\n",
      "Testing Y2: n_est=20, max_depth=15, min_leaf=5, n_feat=0.7\n",
      "  Validation RMSE Y2: 2.1552\n",
      "Testing Y2: n_est=20, max_depth=15, min_leaf=5, n_feat=1.0\n",
      "  Validation RMSE Y2: 1.9844\n",
      "Testing Y2: n_est=20, max_depth=15, min_leaf=10, n_feat=0.5\n",
      "  Validation RMSE Y2: 2.2285\n",
      "Testing Y2: n_est=20, max_depth=15, min_leaf=10, n_feat=0.7\n",
      "  Validation RMSE Y2: 2.2090\n",
      "Testing Y2: n_est=20, max_depth=15, min_leaf=10, n_feat=1.0\n",
      "  Validation RMSE Y2: 1.9233\n",
      "Testing Y2: n_est=20, max_depth=None, min_leaf=1, n_feat=0.5\n",
      "  Validation RMSE Y2: 2.0647\n",
      "Testing Y2: n_est=20, max_depth=None, min_leaf=1, n_feat=0.7\n",
      "  Validation RMSE Y2: 1.9548\n",
      "Testing Y2: n_est=20, max_depth=None, min_leaf=1, n_feat=1.0\n",
      "  Validation RMSE Y2: 1.8049\n",
      "Testing Y2: n_est=20, max_depth=None, min_leaf=5, n_feat=0.5\n",
      "  Validation RMSE Y2: 2.1535\n",
      "Testing Y2: n_est=20, max_depth=None, min_leaf=5, n_feat=0.7\n",
      "  Validation RMSE Y2: 2.1552\n",
      "Testing Y2: n_est=20, max_depth=None, min_leaf=5, n_feat=1.0\n",
      "  Validation RMSE Y2: 1.9844\n",
      "Testing Y2: n_est=20, max_depth=None, min_leaf=10, n_feat=0.5\n",
      "  Validation RMSE Y2: 2.2285\n",
      "Testing Y2: n_est=20, max_depth=None, min_leaf=10, n_feat=0.7\n",
      "  Validation RMSE Y2: 2.2090\n",
      "Testing Y2: n_est=20, max_depth=None, min_leaf=10, n_feat=1.0\n",
      "  Validation RMSE Y2: 1.9233\n",
      "Testing Y2: n_est=50, max_depth=5, min_leaf=1, n_feat=0.5\n",
      "  Validation RMSE Y2: 2.0774\n",
      "Testing Y2: n_est=50, max_depth=5, min_leaf=1, n_feat=0.7\n",
      "  Validation RMSE Y2: 1.9639\n",
      "Testing Y2: n_est=50, max_depth=5, min_leaf=1, n_feat=1.0\n",
      "  Validation RMSE Y2: 1.7728\n",
      "Testing Y2: n_est=50, max_depth=5, min_leaf=5, n_feat=0.5\n",
      "  Validation RMSE Y2: 2.1794\n",
      "Testing Y2: n_est=50, max_depth=5, min_leaf=5, n_feat=0.7\n",
      "  Validation RMSE Y2: 2.0593\n",
      "Testing Y2: n_est=50, max_depth=5, min_leaf=5, n_feat=1.0\n",
      "  Validation RMSE Y2: 1.8553\n",
      "Testing Y2: n_est=50, max_depth=5, min_leaf=10, n_feat=0.5\n",
      "  Validation RMSE Y2: 2.2266\n",
      "Testing Y2: n_est=50, max_depth=5, min_leaf=10, n_feat=0.7\n",
      "  Validation RMSE Y2: 2.0949\n",
      "Testing Y2: n_est=50, max_depth=5, min_leaf=10, n_feat=1.0\n",
      "  Validation RMSE Y2: 1.8763\n",
      "Testing Y2: n_est=50, max_depth=10, min_leaf=1, n_feat=0.5\n",
      "  Validation RMSE Y2: 2.0911\n",
      "Testing Y2: n_est=50, max_depth=10, min_leaf=1, n_feat=0.7\n",
      "  Validation RMSE Y2: 1.8984\n",
      "Testing Y2: n_est=50, max_depth=10, min_leaf=1, n_feat=1.0\n",
      "  Validation RMSE Y2: 1.8165\n",
      "Testing Y2: n_est=50, max_depth=10, min_leaf=5, n_feat=0.5\n",
      "  Validation RMSE Y2: 2.1952\n",
      "Testing Y2: n_est=50, max_depth=10, min_leaf=5, n_feat=0.7\n",
      "  Validation RMSE Y2: 2.0651\n",
      "Testing Y2: n_est=50, max_depth=10, min_leaf=5, n_feat=1.0\n",
      "  Validation RMSE Y2: 1.9744\n",
      "Testing Y2: n_est=50, max_depth=10, min_leaf=10, n_feat=0.5\n",
      "  Validation RMSE Y2: 2.2507\n",
      "Testing Y2: n_est=50, max_depth=10, min_leaf=10, n_feat=0.7\n",
      "  Validation RMSE Y2: 2.1056\n",
      "Testing Y2: n_est=50, max_depth=10, min_leaf=10, n_feat=1.0\n",
      "  Validation RMSE Y2: 1.9111\n",
      "Testing Y2: n_est=50, max_depth=15, min_leaf=1, n_feat=0.5\n",
      "  Validation RMSE Y2: 2.0847\n",
      "Testing Y2: n_est=50, max_depth=15, min_leaf=1, n_feat=0.7\n",
      "  Validation RMSE Y2: 1.8954\n",
      "Testing Y2: n_est=50, max_depth=15, min_leaf=1, n_feat=1.0\n",
      "  Validation RMSE Y2: 1.8019\n",
      "Testing Y2: n_est=50, max_depth=15, min_leaf=5, n_feat=0.5\n",
      "  Validation RMSE Y2: 2.1952\n",
      "Testing Y2: n_est=50, max_depth=15, min_leaf=5, n_feat=0.7\n",
      "  Validation RMSE Y2: 2.0656\n",
      "Testing Y2: n_est=50, max_depth=15, min_leaf=5, n_feat=1.0\n",
      "  Validation RMSE Y2: 1.9744\n",
      "Testing Y2: n_est=50, max_depth=15, min_leaf=10, n_feat=0.5\n",
      "  Validation RMSE Y2: 2.2507\n",
      "Testing Y2: n_est=50, max_depth=15, min_leaf=10, n_feat=0.7\n",
      "  Validation RMSE Y2: 2.1053\n",
      "Testing Y2: n_est=50, max_depth=15, min_leaf=10, n_feat=1.0\n",
      "  Validation RMSE Y2: 1.9111\n",
      "Testing Y2: n_est=50, max_depth=None, min_leaf=1, n_feat=0.5\n",
      "  Validation RMSE Y2: 2.0847\n",
      "Testing Y2: n_est=50, max_depth=None, min_leaf=1, n_feat=0.7\n",
      "  Validation RMSE Y2: 1.8954\n",
      "Testing Y2: n_est=50, max_depth=None, min_leaf=1, n_feat=1.0\n",
      "  Validation RMSE Y2: 1.8019\n",
      "Testing Y2: n_est=50, max_depth=None, min_leaf=5, n_feat=0.5\n",
      "  Validation RMSE Y2: 2.1952\n",
      "Testing Y2: n_est=50, max_depth=None, min_leaf=5, n_feat=0.7\n",
      "  Validation RMSE Y2: 2.0656\n",
      "Testing Y2: n_est=50, max_depth=None, min_leaf=5, n_feat=1.0\n",
      "  Validation RMSE Y2: 1.9744\n",
      "Testing Y2: n_est=50, max_depth=None, min_leaf=10, n_feat=0.5\n",
      "  Validation RMSE Y2: 2.2507\n",
      "Testing Y2: n_est=50, max_depth=None, min_leaf=10, n_feat=0.7\n",
      "  Validation RMSE Y2: 2.1053\n",
      "Testing Y2: n_est=50, max_depth=None, min_leaf=10, n_feat=1.0\n",
      "  Validation RMSE Y2: 1.9111\n",
      "\n",
      "Optimal Hyperparameters for Y2: {'num_estimators': 50, 'max_depth': 5, 'min_samples_leaf': 1, 'num_features': 1.0}\n",
      "Best Validation RMSE for Y2: 1.7728\n"
     ]
    }
   ],
   "source": [
    "#(you are training two models, so you need to different sets of hyperparameters for each)\n",
    "#your code goes here\n",
    "param_grid = {\n",
    "    'num_estimators': [10, 20, 50],\n",
    "    'max_depth': [5, 10, 15, None],\n",
    "    'min_samples_leaf': [1, 5, 10],\n",
    "    'num_features': [0.5, 0.7, 1.0]  # Fraction of features to consider\n",
    "}\n",
    "\n",
    "best_rmse_y1 = float('inf')\n",
    "best_params_y1 = {}\n",
    "\n",
    "# Iterate through all combinations of hyperparameters\n",
    "for n_est in param_grid['num_estimators']:\n",
    "    for m_depth in param_grid['max_depth']:\n",
    "        for min_leaf in param_grid['min_samples_leaf']:\n",
    "            for n_feat in param_grid['num_features']:\n",
    "                print(f\"Testing Y1: n_est={n_est}, max_depth={m_depth}, min_leaf={min_leaf}, n_feat={n_feat}\")\n",
    "                rf_model = RandomForest(\n",
    "                    num_estimators=n_est,\n",
    "                    max_depth=m_depth,\n",
    "                    min_samples_leaf=min_leaf,\n",
    "                    num_features=n_feat,\n",
    "                    random_state=42 # Consistent random state for the whole RF\n",
    "                )\n",
    "                rf_model.fit(X_train_full, y1_train_full)\n",
    "                y1_pred_val = rf_model.predict(X_val)\n",
    "                current_rmse_y1 = rmse(y1_val, y1_pred_val)\n",
    "\n",
    "                if current_rmse_y1 < best_rmse_y1:\n",
    "                    best_rmse_y1 = current_rmse_y1\n",
    "                    best_params_y1 = {\n",
    "                        'num_estimators': n_est,\n",
    "                        'max_depth': m_depth,\n",
    "                        'min_samples_leaf': min_leaf,\n",
    "                        'num_features': n_feat\n",
    "                    }\n",
    "                print(f\"  Validation RMSE Y1: {current_rmse_y1:.4f}\")\n",
    "\n",
    "print(f\"\\nOptimal Hyperparameters for Y1: {best_params_y1}\")\n",
    "print(f\"Best Validation RMSE for Y1: {best_rmse_y1:.4f}\")\n",
    "\n",
    "\n",
    "print(\"\\n--- Hyperparameter Tuning for Y2 (Cooling Load) ---\")\n",
    "best_rmse_y2 = float('inf')\n",
    "best_params_y2 = {}\n",
    "\n",
    "# Iterate through all combinations of hyperparameters for Y2\n",
    "for n_est in param_grid['num_estimators']:\n",
    "    for m_depth in param_grid['max_depth']:\n",
    "        for min_leaf in param_grid['min_samples_leaf']:\n",
    "            for n_feat in param_grid['num_features']:\n",
    "                print(f\"Testing Y2: n_est={n_est}, max_depth={m_depth}, min_leaf={min_leaf}, n_feat={n_feat}\")\n",
    "                rf_model = RandomForest(\n",
    "                    num_estimators=n_est,\n",
    "                    max_depth=m_depth,\n",
    "                    min_samples_leaf=min_leaf,\n",
    "                    num_features=n_feat,\n",
    "                    random_state=42\n",
    "                )\n",
    "                rf_model.fit(X_train_full, y2_train_full)\n",
    "                y2_pred_val = rf_model.predict(X_val)\n",
    "                current_rmse_y2 = rmse(y2_val, y2_pred_val)\n",
    "\n",
    "                if current_rmse_y2 < best_rmse_y2:\n",
    "                    best_rmse_y2 = current_rmse_y2\n",
    "                    best_params_y2 = {\n",
    "                        'num_estimators': n_est,\n",
    "                        'max_depth': m_depth,\n",
    "                        'min_samples_leaf': min_leaf,\n",
    "                        'num_features': n_feat\n",
    "                    }\n",
    "                print(f\"  Validation RMSE Y2: {current_rmse_y2:.4f}\")\n",
    "\n",
    "print(f\"\\nOptimal Hyperparameters for Y2: {best_params_y2}\")\n",
    "print(f\"Best Validation RMSE for Y2: {best_rmse_y2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cdc2e42",
   "metadata": {},
   "source": [
    "Train a final random forest using your optimal hyperparameters and both the training and validation sets. Predict for the datapoints in the testing set and evaluate your final predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4d1c32b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training final model for Y1 (Heating Load)...\n",
      "Final Test RMSE for Y1 (Heating Load): 0.4483\n",
      "\n",
      "Training final model for Y2 (Cooling Load)...\n",
      "Final Test RMSE for Y2 (Cooling Load): 1.9465\n"
     ]
    }
   ],
   "source": [
    "#your code goes here\n",
    "# Combine training and validation sets for final training\n",
    "X_train_val = np.vstack((X_train_full, X_val))\n",
    "y1_train_val = np.concatenate((y1_train_full, y1_val))\n",
    "y2_train_val = np.concatenate((y2_train_full, y2_val))\n",
    "\n",
    "# Train final Random Forest model for Y1 with optimal hyperparameters\n",
    "print(\"\\nTraining final model for Y1 (Heating Load)...\")\n",
    "final_rf_y1 = RandomForest(random_state=42, **best_params_y1)\n",
    "final_rf_y1.fit(X_train_val, y1_train_val)\n",
    "\n",
    "# Predict on the testing set for Y1\n",
    "y1_pred_test = final_rf_y1.predict(X_test)\n",
    "final_rmse_y1 = rmse(y1_test, y1_pred_test)\n",
    "print(f\"Final Test RMSE for Y1 (Heating Load): {final_rmse_y1:.4f}\")\n",
    "\n",
    "# Train final Random Forest model for Y2 with optimal hyperparameters\n",
    "print(\"\\nTraining final model for Y2 (Cooling Load)...\")\n",
    "final_rf_y2 = RandomForest(random_state=42, **best_params_y2)\n",
    "final_rf_y2.fit(X_train_val, y2_train_val)\n",
    "\n",
    "# Predict on the testing set for Y2\n",
    "y2_pred_test = final_rf_y2.predict(X_test)\n",
    "final_rmse_y2 = rmse(y2_test, y2_pred_test)\n",
    "print(f\"Final Test RMSE for Y2 (Cooling Load): {final_rmse_y2:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
